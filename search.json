[
  {
    "objectID": "2024/weeks/week01/vector-space.html",
    "href": "2024/weeks/week01/vector-space.html",
    "title": "Vector Spaces: The Foundation of Linear Algebra",
    "section": "",
    "text": "Session 3 delves into vector spaces and subspaces, introducing essential concepts such as basis and dimension. These ideas lay the groundwork for understanding linear transformations and their matrix representations.\n\n\n\n\n\n\nKey Concept\n\n\n\nVector spaces and subspaces form the foundation of linear transformations, which are represented as matrices in computational applications.\n\n\n\n\n\nA vector space is a collection of objects (vectors) that can be added together and multiplied by scalars, following certain rules. Though vectors are often represented as column matrices, vector spaces can encompass a wide range of objects, such as matrices or functions.\nHere are the eight axioms that must hold for any set to be considered a vector space. Let\\(V\\) be a vector space, and let\\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\) and\\(c, d \\in \\mathbb{R}\\) (or any field).\n\n\n\nClosure under Addition For any two vectors\\(\\mathbf{u}, \\mathbf{v} \\in V\\), their sum\\(\\mathbf{u} + \\mathbf{v}\\) must also be in\\(V\\).\n\n\n\\[\n\\mathbf{u} + \\mathbf{v} \\in V\n\\]\n\n\n\nClosure under Scalar Multiplication For any scalar\\(c \\in \\mathbb{R}\\) and any vector\\(\\mathbf{v} \\in V\\), the scalar multiple\\(c\\mathbf{v}\\) must also be in\\(V\\).\n\n\n\\[\nc\\mathbf{v} \\in V\n\\]\n\n\n\nAssociativity of Vector Addition For all vectors\\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\), the sum of vectors is associative.\n\n\n\\[\n(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n\\]\n\n\n\nCommutativity of Vector Addition For all vectors\\(\\mathbf{u}, \\mathbf{v} \\in V\\), vector addition is commutative.\n\n\n\\[\n\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n\\]\n\n\n\nExistence of the Zero Vector There exists a zero vector\\(\\mathbf{0} \\in V\\) such that, for any vector\\(\\mathbf{v} \\in V\\), the following holds:\n\n\n\\[\n\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n\\]\n\n\n\nExistence of Additive Inverses For each vector\\(\\mathbf{v} \\in V\\), there exists a vector\\(-\\mathbf{v} \\in V\\) such that:\n\n\n\\[\n\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n\\]\n\n\n\nDistributivity of Scalar Multiplication with Respect to Vector Addition For all scalars\\(c \\in \\mathbb{R}\\) and vectors\\(\\mathbf{u}, \\mathbf{v} \\in V\\), scalar multiplication distributes over vector addition:\n\n\n\\[\nc(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\n\\]\n\n\n\nDistributivity of Scalar Multiplication with Respect to Scalar Addition For all scalars\\(c, d \\in \\mathbb{R}\\) and any vector\\(\\mathbf{v} \\in V\\), scalar multiplication distributes over scalar addition:\n\n\n\\[\n(c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\n\\]\n\n\n\nCompatibility of Scalar Multiplication For any scalars\\(c, d \\in \\mathbb{R}\\) and any vector\\(\\mathbf{v} \\in V\\):\n\n\n\\[\nc(d\\mathbf{v}) = (cd)\\mathbf{v}\n\\]\n\n\n\nIdentity Element of Scalar Multiplication The scalar 1 acts as the identity element for scalar multiplication. For any vector\\(\\mathbf{v} \\in V\\):\n\n\n\\[\n1 \\mathbf{v} = \\mathbf{v}\n\\]\n\nThese axioms ensure that vectors and scalars interact in a consistent and structured way, forming the basis for many applications of linear algebra in various fields, including computer science, engineering, and physics.\n\n\n\n\n\n\nKey Concept\n\n\n\nClosure under linear combinations: Any linear combination of vectors within a space must also belong to that space, ensuring completeness in terms of operations.\n\n\n\n\n\nEuclidean spaces: \\(\\mathbb{R}^n\\), where vectors are represented as ordered lists of real numbers.\nMatrix spaces: All matrices of a specific size, e.g., the space of all \\(3 \\times 3\\) matrices.\nFunction spaces: Spaces formed by collections of functions, such as continuous functions over a given interval.\n\n\n\n\n\n\nA subspace is a subset of vectors within a vector space that forms its own vector space. Subspaces must contain the zero vector and be closed under vector addition and scalar multiplication.\n\n\n\n\n\n\nKey Concept\n\n\n\nSubspaces are smaller vector spaces within a larger space, inheriting all vector space properties, including the presence of the zero vector and closure under linear combinations.\n\n\n\n\n\nPlanes and lines through the origin in \\(\\mathbb{R}^3\\) are subspaces of \\(\\mathbb{R}^3\\).\nThe set containing only the zero vector is a subspace of any vector space.\nUpper triangular matrices form a subspace within the space of all matrices of the same size.\n\n\n\n\n\n\nEvery matrix \\(A\\) has four fundamental subspaces that offer insight into its behavior as a linear transformation:\n\nColumn space \\(C(A)\\): The set of all linear combinations of the columns of \\(A\\). This captures the range of the transformation \\(A\\mathbf{x}\\).\nRow space \\(C(A^T)\\): The set of all linear combinations of the rows of \\(A\\), equivalent to the column space of \\(A^T\\).\nNullspace \\(N(A)\\): The set of vectors \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = 0\\). These vectors are mapped to zero by the transformation.\nLeft nullspace \\(N(A^T)\\): The set of vectors \\(\\mathbf{y}\\) such that \\(A^T\\mathbf{y} = 0\\), equivalent to the nullspace of \\(A^T\\).\n\n\n\n\n\n\n\nKey Concept\n\n\n\nOrthogonality Relationships:\n- The row space and nullspace are orthogonal to each other.\n- The column space and left nullspace are orthogonal to each other.\n\n\n\n\n\n\nA basis of a vector space is a set of linearly independent vectors that span the entire space. The dimension of the space is the number of vectors in any basis, indicating how many independent directions or components the space has.\n\n\n\n\n\n\nKey Concept\n\n\n\nLinear Independence and Spanning:\n- Linear independence ensures that no vector in the basis can be expressed as a linear combination of others. - Spanning ensures that every vector in the space can be represented as a combination of the basis vectors.\n\n\n\n\n\nThe standard basis for \\(\\mathbb{R}^3\\) is \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\), and the dimension of \\(\\mathbb{R}^3\\) is 3.\nThe rank of a matrix equals the dimension of its column space.\nThe nullity of a matrix equals the dimension of its nullspace, representing the number of free variables in \\(A\\mathbf{x} = 0\\).\n\n\n\n\n\n\nThe elimination process, as described in session 2, can help find bases for the row space, column space, and nullspace of a matrix. By transforming a matrix into its reduced row echelon form (RREF), we can: - Identify the pivot columns to form a basis for the column space. - Use corresponding rows from the RREF to form a basis for the row space. - Solve \\(R\\mathbf{x} = 0\\) (where \\(R\\) is the RREF) to find a basis for the nullspace.\n\n\n\n\nSection 3 serves as a stepping stone for more advanced linear algebra concepts:\n\nLinear Transformations: Functions between vector spaces that preserve linearity. Matrices represent such transformations, and the four fundamental subspaces offer insights into their structure.\nOrthogonality and Projections: Orthogonality plays a central role in finding the closest vector in a subspace to a given vector, leading to the concept of projections.\nEigenvalues and Eigenvectors: These are special vectors that are mapped to scalar multiples of themselves by a linear transformation. They reveal the long-term behavior of systems in dynamic applications.\n\n\n\n\n\nThe study of vector spaces and subspaces, along with concepts like basis and dimension, provides a foundational understanding of the structure behind linear transformations. These core ideas are essential for anyone working in linear algebra, forming the backbone for analyzing problems across multiple fields, from computer science to engineering.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Essentials of Vector Spaces"
    ]
  },
  {
    "objectID": "2024/weeks/week01/vector-space.html#vector-spaces-the-foundation-of-linear-algebra",
    "href": "2024/weeks/week01/vector-space.html#vector-spaces-the-foundation-of-linear-algebra",
    "title": "Vector Spaces: The Foundation of Linear Algebra",
    "section": "",
    "text": "Session 3 delves into vector spaces and subspaces, introducing essential concepts such as basis and dimension. These ideas lay the groundwork for understanding linear transformations and their matrix representations.\n\n\n\n\n\n\nKey Concept\n\n\n\nVector spaces and subspaces form the foundation of linear transformations, which are represented as matrices in computational applications.\n\n\n\n\n\nA vector space is a collection of objects (vectors) that can be added together and multiplied by scalars, following certain rules. Though vectors are often represented as column matrices, vector spaces can encompass a wide range of objects, such as matrices or functions.\nHere are the eight axioms that must hold for any set to be considered a vector space. Let\\(V\\) be a vector space, and let\\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\) and\\(c, d \\in \\mathbb{R}\\) (or any field).\n\n\n\nClosure under Addition For any two vectors\\(\\mathbf{u}, \\mathbf{v} \\in V\\), their sum\\(\\mathbf{u} + \\mathbf{v}\\) must also be in\\(V\\).\n\n\n\\[\n\\mathbf{u} + \\mathbf{v} \\in V\n\\]\n\n\n\nClosure under Scalar Multiplication For any scalar\\(c \\in \\mathbb{R}\\) and any vector\\(\\mathbf{v} \\in V\\), the scalar multiple\\(c\\mathbf{v}\\) must also be in\\(V\\).\n\n\n\\[\nc\\mathbf{v} \\in V\n\\]\n\n\n\nAssociativity of Vector Addition For all vectors\\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\), the sum of vectors is associative.\n\n\n\\[\n(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n\\]\n\n\n\nCommutativity of Vector Addition For all vectors\\(\\mathbf{u}, \\mathbf{v} \\in V\\), vector addition is commutative.\n\n\n\\[\n\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\n\\]\n\n\n\nExistence of the Zero Vector There exists a zero vector\\(\\mathbf{0} \\in V\\) such that, for any vector\\(\\mathbf{v} \\in V\\), the following holds:\n\n\n\\[\n\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\n\\]\n\n\n\nExistence of Additive Inverses For each vector\\(\\mathbf{v} \\in V\\), there exists a vector\\(-\\mathbf{v} \\in V\\) such that:\n\n\n\\[\n\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\n\\]\n\n\n\nDistributivity of Scalar Multiplication with Respect to Vector Addition For all scalars\\(c \\in \\mathbb{R}\\) and vectors\\(\\mathbf{u}, \\mathbf{v} \\in V\\), scalar multiplication distributes over vector addition:\n\n\n\\[\nc(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\n\\]\n\n\n\nDistributivity of Scalar Multiplication with Respect to Scalar Addition For all scalars\\(c, d \\in \\mathbb{R}\\) and any vector\\(\\mathbf{v} \\in V\\), scalar multiplication distributes over scalar addition:\n\n\n\\[\n(c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\n\\]\n\n\n\nCompatibility of Scalar Multiplication For any scalars\\(c, d \\in \\mathbb{R}\\) and any vector\\(\\mathbf{v} \\in V\\):\n\n\n\\[\nc(d\\mathbf{v}) = (cd)\\mathbf{v}\n\\]\n\n\n\nIdentity Element of Scalar Multiplication The scalar 1 acts as the identity element for scalar multiplication. For any vector\\(\\mathbf{v} \\in V\\):\n\n\n\\[\n1 \\mathbf{v} = \\mathbf{v}\n\\]\n\nThese axioms ensure that vectors and scalars interact in a consistent and structured way, forming the basis for many applications of linear algebra in various fields, including computer science, engineering, and physics.\n\n\n\n\n\n\nKey Concept\n\n\n\nClosure under linear combinations: Any linear combination of vectors within a space must also belong to that space, ensuring completeness in terms of operations.\n\n\n\n\n\nEuclidean spaces: \\(\\mathbb{R}^n\\), where vectors are represented as ordered lists of real numbers.\nMatrix spaces: All matrices of a specific size, e.g., the space of all \\(3 \\times 3\\) matrices.\nFunction spaces: Spaces formed by collections of functions, such as continuous functions over a given interval.\n\n\n\n\n\n\nA subspace is a subset of vectors within a vector space that forms its own vector space. Subspaces must contain the zero vector and be closed under vector addition and scalar multiplication.\n\n\n\n\n\n\nKey Concept\n\n\n\nSubspaces are smaller vector spaces within a larger space, inheriting all vector space properties, including the presence of the zero vector and closure under linear combinations.\n\n\n\n\n\nPlanes and lines through the origin in \\(\\mathbb{R}^3\\) are subspaces of \\(\\mathbb{R}^3\\).\nThe set containing only the zero vector is a subspace of any vector space.\nUpper triangular matrices form a subspace within the space of all matrices of the same size.\n\n\n\n\n\n\nEvery matrix \\(A\\) has four fundamental subspaces that offer insight into its behavior as a linear transformation:\n\nColumn space \\(C(A)\\): The set of all linear combinations of the columns of \\(A\\). This captures the range of the transformation \\(A\\mathbf{x}\\).\nRow space \\(C(A^T)\\): The set of all linear combinations of the rows of \\(A\\), equivalent to the column space of \\(A^T\\).\nNullspace \\(N(A)\\): The set of vectors \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = 0\\). These vectors are mapped to zero by the transformation.\nLeft nullspace \\(N(A^T)\\): The set of vectors \\(\\mathbf{y}\\) such that \\(A^T\\mathbf{y} = 0\\), equivalent to the nullspace of \\(A^T\\).\n\n\n\n\n\n\n\nKey Concept\n\n\n\nOrthogonality Relationships:\n- The row space and nullspace are orthogonal to each other.\n- The column space and left nullspace are orthogonal to each other.\n\n\n\n\n\n\nA basis of a vector space is a set of linearly independent vectors that span the entire space. The dimension of the space is the number of vectors in any basis, indicating how many independent directions or components the space has.\n\n\n\n\n\n\nKey Concept\n\n\n\nLinear Independence and Spanning:\n- Linear independence ensures that no vector in the basis can be expressed as a linear combination of others. - Spanning ensures that every vector in the space can be represented as a combination of the basis vectors.\n\n\n\n\n\nThe standard basis for \\(\\mathbb{R}^3\\) is \\(\\{(1,0,0), (0,1,0), (0,0,1)\\}\\), and the dimension of \\(\\mathbb{R}^3\\) is 3.\nThe rank of a matrix equals the dimension of its column space.\nThe nullity of a matrix equals the dimension of its nullspace, representing the number of free variables in \\(A\\mathbf{x} = 0\\).\n\n\n\n\n\n\nThe elimination process, as described in session 2, can help find bases for the row space, column space, and nullspace of a matrix. By transforming a matrix into its reduced row echelon form (RREF), we can: - Identify the pivot columns to form a basis for the column space. - Use corresponding rows from the RREF to form a basis for the row space. - Solve \\(R\\mathbf{x} = 0\\) (where \\(R\\) is the RREF) to find a basis for the nullspace.\n\n\n\n\nSection 3 serves as a stepping stone for more advanced linear algebra concepts:\n\nLinear Transformations: Functions between vector spaces that preserve linearity. Matrices represent such transformations, and the four fundamental subspaces offer insights into their structure.\nOrthogonality and Projections: Orthogonality plays a central role in finding the closest vector in a subspace to a given vector, leading to the concept of projections.\nEigenvalues and Eigenvectors: These are special vectors that are mapped to scalar multiples of themselves by a linear transformation. They reveal the long-term behavior of systems in dynamic applications.\n\n\n\n\n\nThe study of vector spaces and subspaces, along with concepts like basis and dimension, provides a foundational understanding of the structure behind linear transformations. These core ideas are essential for anyone working in linear algebra, forming the backbone for analyzing problems across multiple fields, from computer science to engineering.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Essentials of Vector Spaces"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/page3.html",
    "href": "2024/weeks/AddonCourse/page3.html",
    "title": "üóìÔ∏è Add-On Course - LaTeX Course for Professionals",
    "section": "",
    "text": "This LaTeX course covers essential topics for creating professional engineering documentation. Participants will learn about document structure, typesetting equations, creating tables and figures, and managing bibliographies. The course includes practical sessions on using LaTeX templates and advanced bibliography management with biblatex.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/page3.html#lecture-slides",
    "href": "2024/weeks/AddonCourse/page3.html#lecture-slides",
    "title": "üóìÔ∏è Add-On Course - LaTeX Course for Professionals",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\n\n\n\n\nüé• Looking for lecture recordings? You can only find those on Moodle.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/page3.html#coursework",
    "href": "2024/weeks/AddonCourse/page3.html#coursework",
    "title": "üóìÔ∏è Add-On Course - LaTeX Course for Professionals",
    "section": "‚úçÔ∏è Coursework",
    "text": "‚úçÔ∏è Coursework\nüöß Come back after the lecture to read the coursework instructions for the week üöß",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/page3.html#recommended-reading",
    "href": "2024/weeks/AddonCourse/page3.html#recommended-reading",
    "title": "üóìÔ∏è Add-On Course - LaTeX Course for Professionals",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/page3.html#communication",
    "href": "2024/weeks/AddonCourse/page3.html#communication",
    "title": "üóìÔ∏è Add-On Course - LaTeX Course for Professionals",
    "section": "üìü Communication",
    "text": "üìü Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Slack",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "Welcome to Mastering LaTeX for Engineering Professionals, a hands-on course designed to equip you with the skills to create professional-grade documents using LaTeX. Whether you‚Äôre a seasoned engineer or just starting out, this course will guide you through the essentials of LaTeX, from fundamental syntax to advanced formatting techniques, enabling you to create visually stunning documents with technical precision.\n\n\n\nIn engineering and scientific fields, clear, well-formatted documentation is essential. LaTeX is a powerful tool used by professionals worldwide for typesetting complex documents, such as research papers, technical reports, resumes, and presentations. This course will empower you to:\n\nCreate visually appealing documents that meet industry standards for technical writing and publishing.\nHandle complex equations, tables, and figures with ease and precision.\nAutomate formatting tasks, so you can focus on content rather than layout.\nCollaborate seamlessly with team members using version control tools and platforms like Overleaf and Git.\n\nJoin us to gain a skill that will enhance your documentation and set you apart in your professional work.\n\n\n\n\nBy the end of this course, you will be able to:\n\nUnderstand the basics of LaTeX syntax and structure.\nFormat documents, tables, and figures for professional-quality results.\nTypeset mathematical expressions, formulas, and complex equations.\nCreate structured reports, articles, and research papers.\nUse LaTeX templates for a variety of document types, including resumes, technical reports, and presentations.\nIntegrate LaTeX with tools like Overleaf for collaborative writing.\n\n\n\n\n\nThis course is ideal for:\n\nEngineers and scientists looking to enhance their documentation skills.\nGraduate students and academics preparing research papers, theses, and presentations.\nIndustry professionals who regularly create technical reports and specifications.\nAnyone interested in producing professional-quality documents with a technical focus.\n\n\n\n\n\n\n\n\nWhy LaTeX? Overview and Applications\nInstalling and Setting Up LaTeX\nUnderstanding LaTeX Syntax and Structure\n\n\n\n\n\nText Formatting and Page Layouts\nSections, Lists, and Tables\nAdding Images and Figures\n\n\n\n\n\nWriting Equations and Formulas\nThe Power of LaTeX for Complex Math\nAligning and Referencing Equations\n\n\n\n\n\nCreating Bibliographies with BibTeX\nDocument Class Options for Reports, Articles, and Presentations\nCreating and Using Custom Templates\n\n\n\n\n\nUsing LaTeX on Overleaf for Collaboration\nIntegrating LaTeX with Git for Version Control\nBest Practices for Document Collaboration\n\n\n\n\n\n\n\nCourse Notes: Detailed PDFs with examples and explanations.\nTemplates: Pre-made LaTeX templates for various document types, including resumes, articles, and reports.\nCode Repository: Access to code examples and exercises on GitHub.\nDiscussion Forum: A dedicated forum for questions, discussions, and collaboration.\n\n\n\n\n\n\nDuration: 4 weeks\nFormat: Online lectures with downloadable exercises and templates\nAssignments: Weekly hands-on assignments to reinforce skills\nProject: Final project to create a professional document using LaTeX\n\n\n\n\n\nReady to transform your documentation skills? Enroll today and unlock the power of LaTeX for professional engineering documentation. Create documents that reflect your technical expertise with precision, elegance, and impact.\n\nRegister Now",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#why-learn-latex",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#why-learn-latex",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "In engineering and scientific fields, clear, well-formatted documentation is essential. LaTeX is a powerful tool used by professionals worldwide for typesetting complex documents, such as research papers, technical reports, resumes, and presentations. This course will empower you to:\n\nCreate visually appealing documents that meet industry standards for technical writing and publishing.\nHandle complex equations, tables, and figures with ease and precision.\nAutomate formatting tasks, so you can focus on content rather than layout.\nCollaborate seamlessly with team members using version control tools and platforms like Overleaf and Git.\n\nJoin us to gain a skill that will enhance your documentation and set you apart in your professional work.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#course-objectives",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#course-objectives",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "By the end of this course, you will be able to:\n\nUnderstand the basics of LaTeX syntax and structure.\nFormat documents, tables, and figures for professional-quality results.\nTypeset mathematical expressions, formulas, and complex equations.\nCreate structured reports, articles, and research papers.\nUse LaTeX templates for a variety of document types, including resumes, technical reports, and presentations.\nIntegrate LaTeX with tools like Overleaf for collaborative writing.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#target-audience",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#target-audience",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "This course is ideal for:\n\nEngineers and scientists looking to enhance their documentation skills.\nGraduate students and academics preparing research papers, theses, and presentations.\nIndustry professionals who regularly create technical reports and specifications.\nAnyone interested in producing professional-quality documents with a technical focus.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#course-outline",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#course-outline",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "Why LaTeX? Overview and Applications\nInstalling and Setting Up LaTeX\nUnderstanding LaTeX Syntax and Structure\n\n\n\n\n\nText Formatting and Page Layouts\nSections, Lists, and Tables\nAdding Images and Figures\n\n\n\n\n\nWriting Equations and Formulas\nThe Power of LaTeX for Complex Math\nAligning and Referencing Equations\n\n\n\n\n\nCreating Bibliographies with BibTeX\nDocument Class Options for Reports, Articles, and Presentations\nCreating and Using Custom Templates\n\n\n\n\n\nUsing LaTeX on Overleaf for Collaboration\nIntegrating LaTeX with Git for Version Control\nBest Practices for Document Collaboration",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#course-resources",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#course-resources",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "Course Notes: Detailed PDFs with examples and explanations.\nTemplates: Pre-made LaTeX templates for various document types, including resumes, articles, and reports.\nCode Repository: Access to code examples and exercises on GitHub.\nDiscussion Forum: A dedicated forum for questions, discussions, and collaboration.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#course-format",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#course-format",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "Duration: 4 weeks\nFormat: Online lectures with downloadable exercises and templates\nAssignments: Weekly hands-on assignments to reinforce skills\nProject: Final project to create a professional document using LaTeX",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_course.html#enroll-today",
    "href": "2024/weeks/AddonCourse/LaTeX_course.html#enroll-today",
    "title": "Mastering LaTeX for Engineering Professionals",
    "section": "",
    "text": "Ready to transform your documentation skills? Enroll today and unlock the power of LaTeX for professional engineering documentation. Create documents that reflect your technical expertise with precision, elegance, and impact.\n\nRegister Now",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ A Short Course in LaTeX"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page2.html",
    "href": "2024/weeks/week02/page2.html",
    "title": "üóìÔ∏è Module 2 - Vector Space and Its Applications",
    "section": "",
    "text": "In this week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page2.html#lecture-slides",
    "href": "2024/weeks/week02/page2.html#lecture-slides",
    "title": "üóìÔ∏è Module 2 - Vector Space and Its Applications",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\n\n\n\n\nüé• Looking for lecture recordings? You can only find those on Moodle.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page2.html#coursework",
    "href": "2024/weeks/week02/page2.html#coursework",
    "title": "üóìÔ∏è Module 2 - Vector Space and Its Applications",
    "section": "‚úçÔ∏è Coursework",
    "text": "‚úçÔ∏è Coursework\nüöß Come back after the lecture to read the coursework instructions for the week üöß",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page2.html#recommended-reading",
    "href": "2024/weeks/week02/page2.html#recommended-reading",
    "title": "üóìÔ∏è Module 2 - Vector Space and Its Applications",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02"
    ]
  },
  {
    "objectID": "2024/weeks/week02/page2.html#communication",
    "href": "2024/weeks/week02/page2.html#communication",
    "title": "üóìÔ∏è Module 2 - Vector Space and Its Applications",
    "section": "üìü Communication",
    "text": "üìü Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Slack",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html",
    "href": "2024/weeks/week02/matrix_with_soln.html",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "",
    "text": "Consider the linear system \\(Ax = b\\), where \\(A\\) is a \\(4 \\times 3\\) matrix constructed in different ways, and \\(b\\) is a given vector defined as:\n\\[\nb = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}\n\\]\nThe goal is to analyze how the construction of \\(A\\) affects the types of solutions available for the system. Specifically, we will explore three configurations of \\(A\\) that lead to:\n\nInfinite solutions\nA unique solution\nNo solution\n\n\n\n\nGenerate Random Integers: Start by creating a random \\(4 \\times 3\\) matrix \\(B\\) filled with single digit integer values.\nAdd Small Noise: Introduce a small random noise (for example,add a very small value) to each element of the matrix \\(B\\). This helps ensure that the rows are not linearly dependent and maintains the rank of the matrix.\n\n\n\n\n\n\n\nCombine Rows: Create a new matrix \\(K1\\) by selecting the first two rows of \\(B\\).\nAdd Linear Combinations: Add additional rows to \\(K1\\) that are linear combinations of the first two rows, for example:\n\nThe sum of the two selected rows.\nA multiple of the two selected rows.\n\nForm Matrix \\(A1\\): Extract the first three columns from \\(K1\\) to form matrix \\(A1\\). The rank of this matrix will be less than 3, indicating that the system \\(Ax = b\\) has infinitely many solutions.\n\n\n\n\n\nSelect Rows: Create a new matrix \\(K2\\) by selecting the first three rows of \\(B\\).\nAdd a New Row: Add an additional row to \\(K2\\) that is a linear combination of the three selected rows.\nForm Matrix \\(A2\\): Extract the first three columns from \\(K2\\) to create matrix \\(A2\\). The rank of this matrix will be exactly 3, which implies that the system \\(Ax = b\\) has a unique solution.\n\n\n\n\n\nSelect Rows: Create matrix \\(A3\\) by selecting the first three rows of \\(B\\).\nAdd an Inconsistent Row: Add a row to \\(A3\\) that cannot be expressed as a linear combination of the first three rows (e.g., a new row with specific coefficients that do not align with the linear span of the first three).\nForm Matrix \\(A3\\): The resulting matrix \\(A3\\) will be inconsistent. This means that the system \\(Ax = b\\) has no solution.\n\n\n\n\n\n\nRank Check: For each constructed matrix \\(A\\), check its rank to understand the nature of the solutions:\n\nFor \\(A1\\), expect a rank less than 3, indicating infinite solutions.\nFor \\(A2\\), expect a rank of 3, indicating a unique solution.\nFor \\(A3\\), expect a rank less than 4, indicating no solution.\n\nSolve the System: Attempt to solve the linear system \\(Ax = b\\) for each case:\n\nFor \\(A1\\), the solution will have a parameterized form due to infinite solutions.\nFor \\(A2\\), a specific unique solution will be found.\nFor \\(A3\\), the system will indicate that no solution exists.\n\n\n\n\n\nWe also computationally solve different types of systems of equations using these inverses. Special emphasis is placed on interpreting the behavior of systems with varying ranks and showing how the pseudo inverse generalizes the concept of matrix inverses.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#step-1-create-a-random-matrix-b",
    "href": "2024/weeks/week02/matrix_with_soln.html#step-1-create-a-random-matrix-b",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "",
    "text": "Generate Random Integers: Start by creating a random \\(4 \\times 3\\) matrix \\(B\\) filled with single digit integer values.\nAdd Small Noise: Introduce a small random noise (for example,add a very small value) to each element of the matrix \\(B\\). This helps ensure that the rows are not linearly dependent and maintains the rank of the matrix.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#step-2-construct-different-cases-of-matrix-a",
    "href": "2024/weeks/week02/matrix_with_soln.html#step-2-construct-different-cases-of-matrix-a",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "",
    "text": "Combine Rows: Create a new matrix \\(K1\\) by selecting the first two rows of \\(B\\).\nAdd Linear Combinations: Add additional rows to \\(K1\\) that are linear combinations of the first two rows, for example:\n\nThe sum of the two selected rows.\nA multiple of the two selected rows.\n\nForm Matrix \\(A1\\): Extract the first three columns from \\(K1\\) to form matrix \\(A1\\). The rank of this matrix will be less than 3, indicating that the system \\(Ax = b\\) has infinitely many solutions.\n\n\n\n\n\nSelect Rows: Create a new matrix \\(K2\\) by selecting the first three rows of \\(B\\).\nAdd a New Row: Add an additional row to \\(K2\\) that is a linear combination of the three selected rows.\nForm Matrix \\(A2\\): Extract the first three columns from \\(K2\\) to create matrix \\(A2\\). The rank of this matrix will be exactly 3, which implies that the system \\(Ax = b\\) has a unique solution.\n\n\n\n\n\nSelect Rows: Create matrix \\(A3\\) by selecting the first three rows of \\(B\\).\nAdd an Inconsistent Row: Add a row to \\(A3\\) that cannot be expressed as a linear combination of the first three rows (e.g., a new row with specific coefficients that do not align with the linear span of the first three).\nForm Matrix \\(A3\\): The resulting matrix \\(A3\\) will be inconsistent. This means that the system \\(Ax = b\\) has no solution.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#step-3-analyze-the-solution-types",
    "href": "2024/weeks/week02/matrix_with_soln.html#step-3-analyze-the-solution-types",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "",
    "text": "Rank Check: For each constructed matrix \\(A\\), check its rank to understand the nature of the solutions:\n\nFor \\(A1\\), expect a rank less than 3, indicating infinite solutions.\nFor \\(A2\\), expect a rank of 3, indicating a unique solution.\nFor \\(A3\\), expect a rank less than 4, indicating no solution.\n\nSolve the System: Attempt to solve the linear system \\(Ax = b\\) for each case:\n\nFor \\(A1\\), the solution will have a parameterized form due to infinite solutions.\nFor \\(A2\\), a specific unique solution will be found.\nFor \\(A3\\), the system will indicate that no solution exists.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#extension-note",
    "href": "2024/weeks/week02/matrix_with_soln.html#extension-note",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "",
    "text": "We also computationally solve different types of systems of equations using these inverses. Special emphasis is placed on interpreting the behavior of systems with varying ranks and showing how the pseudo inverse generalizes the concept of matrix inverses.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#left-inverse",
    "href": "2024/weeks/week02/matrix_with_soln.html#left-inverse",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "Left Inverse",
    "text": "Left Inverse\nIf \\(A\\) is a matrix with full column rank (i.e., the columns of \\(A\\) are linearly independent), a left inverse exists. For a matrix \\(A\\) of dimensions \\(m \\times n\\), where \\(m &gt; n\\), the left inverse is denoted as:\n\\[\nA_{\\text{left}}^{-1} = (A^T A)^{-1} A^T\n\\]\nThis inverse satisfies:\n\\[\nA_{\\text{left}}^{-1} A = I\n\\]\n\nCondition for Existence\n\nFull column rank: All columns are linearly independent.\nMore rows than columns: \\(m &gt; n\\).\n\nWhen the left inverse exists, we can use it to solve a system of equations of the form \\(Ax = b\\).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#right-inverse",
    "href": "2024/weeks/week02/matrix_with_soln.html#right-inverse",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "Right Inverse",
    "text": "Right Inverse\nIf \\(A\\) has full row rank (i.e., the rows of \\(A\\) are linearly independent), a right inverse exists. For a matrix \\(A\\) of dimensions \\(m \\times n\\), where \\(m &lt; n\\), the right inverse is denoted as:\n\\[\nA_{\\text{right}}^{-1} = A^T (A A^T)^{-1}\n\\]\nThis inverse satisfies:\n\\[\nA A_{\\text{right}}^{-1} = I\n\\]\n\nCondition for Existence\n\nFull row rank: All rows are linearly independent.\nMore columns than rows: \\(n &gt; m\\).\n\nWhen the right inverse exists, we can solve \\(Ax = b\\) using this inverse.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#pseudo-inverse",
    "href": "2024/weeks/week02/matrix_with_soln.html#pseudo-inverse",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "Pseudo Inverse",
    "text": "Pseudo Inverse\nThe Moore-Penrose pseudo inverse is a generalization of the matrix inverse. It is denoted as \\(A^+\\) and exists for any matrix, regardless of whether \\(A\\) has full rank. It is used when either the left or right inverse does not exist or when the system is inconsistent or underdetermined.\nThe pseudo inverse is defined as:\n\\[\nA^+ = \\text{argmin} \\|Ax - b\\|_2\n\\]\nIt provides the least-squares solution to the system \\(Ax = b\\), which minimizes the residual when no exact solution exists.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#case-1-infinite-solutions-1",
    "href": "2024/weeks/week02/matrix_with_soln.html#case-1-infinite-solutions-1",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "Case 1: Infinite Solutions",
    "text": "Case 1: Infinite Solutions\nWe construct a matrix \\(A1\\) with linearly dependent rows, which results in infinite solutions to the system \\(A1x = b\\). In this case, the rank of \\(A1\\) is less than the number of columns, indicating an underdetermined system.\nThe pseudo inverse is used to find a least-squares solution.\n\\[\nA1 =\n\\begin{bmatrix}\nB(1,:) \\\\\nB(2,:) \\\\\n\\alpha_1 B(1,:) + \\alpha_2 B(2,:) \\\\\n\\beta_1 B(1,:) + \\beta_2 B(2,:)\n\\end{bmatrix}\n\\]\nwhere \\(B\\) is a random matrix, and \\(\\alpha, \\beta\\) are scalars.\nThe system is underdetermined, and the pseudo inverse provides a solution.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#case-2-unique-solution-1",
    "href": "2024/weeks/week02/matrix_with_soln.html#case-2-unique-solution-1",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "Case 2: Unique Solution",
    "text": "Case 2: Unique Solution\nIn this case, we construct a matrix \\(A2\\) that has full column rank. This ensures a unique solution for \\(A2x = b\\), as the system is neither underdetermined nor overdetermined. The left inverse is applicable here.\n\\[\nA2 =\n\\begin{bmatrix}\nB(1,:) \\\\\nB(2,:) \\\\\nB(3,:) \\\\\n\\alpha_1 B(1,:) + \\alpha_2 B(2,:) + \\alpha_3 B(3,:)\n\\end{bmatrix}\n\\]\nwhere \\(B\\) is a random matrix, and \\(\\alpha\\) are scalars.\nSince the matrix has full column rank, the left inverse and pseudo inverse coincide.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/matrix_with_soln.html#case-3-no-solution-1",
    "href": "2024/weeks/week02/matrix_with_soln.html#case-3-no-solution-1",
    "title": "Analysis of Linear Systems: Exploring Transformation Which Gives a Particular Effect",
    "section": "Case 3: No Solution",
    "text": "Case 3: No Solution\nIn this case, the matrix \\(A3\\) is inconsistent, meaning that there is no exact solution for \\(A3x = b\\). The system is overdetermined (more rows than columns), but the rows are not independent. As a result, the pseudo inverse provides a least-squares solution that minimizes the residual.\n\\[\nA3 =\n\\begin{bmatrix}\nB(1,:) \\\\\nB(2,:) \\\\\nB(3,:) \\\\\n\\gamma_1 B(1,:) + \\gamma_2 B(2,:) + \\gamma_3 B(3,:)\n\\end{bmatrix}\n\\]\nwhere \\(\\gamma\\) are chosen such that the system is inconsistent.\nMatlab code to solve the above system with given selected matrices is given below.\n\nMatlabPython Code\n\n\n% Set a random seed for reproducibility (optional)\nrng(0);\n\n% Step 1: Create a random integer matrix B\nB = randi([1, 10], 4, 3) + 1e-10 * randn(4, 3); % Adding small noise\n\n% Given vector b\nb = [1; 2; 3; 4];\n\n% Case 1: Infinite solutions\n% K1: Combine the first two rows of B with linear combinations\nK1 = [B(1:2, :); [1, 1] * B(1:2, :); [1, 2] * B(1:2, :)];\nA1 = K1(:, 1:3); % Form matrix A1\nfprintf('Case 1: Infinite solutions\\n');\nrankA1 = rank(A1);\nfprintf('Rank of A1: %d\\n', rankA1);\n\n% Solving with pseudo inverse\nx_pseudo1 = pinv(A1) * b;\nfprintf('Pseudo inverse solution for A1:\\n');\ndisp(x_pseudo1);\n\n% Case 2: Unique solution\n% K2: Combine the first three rows of B and add a linear combination\nK2 = [B(1:3, :); [1, 1, 1] * B(1:3, :)];\nA2 = K2(:, 1:3); % Form matrix A2\nfprintf('Case 2: Unique solution\\n');\nrankA2 = rank(A2);\nfprintf('Rank of A2: %d\\n', rankA2);\n\n% Left inverse (A2 has full column rank, so left inverse exists)\nA2_left_inv = inv(A2' * A2) * A2'; % Moore-Penrose will coincide with this\nx_left2 = A2_left_inv * b;\nfprintf('Left inverse solution for A2:\\n');\ndisp(x_left2);\n\n% Pseudo inverse (should coincide with left inverse for full column rank)\nx_pseudo2 = pinv(A2) * b;\nfprintf('Pseudo inverse solution for A2:\\n');\ndisp(x_pseudo2);\n\n% Verify if left inverse coincides with pseudo inverse\nfprintf('Difference between left inverse and pseudo inverse for A2:\\n');\ndisp(norm(x_left2 - x_pseudo2));\n\n% Case 3: No solution\n% A3: Combine the first three rows of B and add an inconsistent row\nA3 = [B(1:3, :); [0.3, 0.5, 1] * B(1:3, :)];\nfprintf('Case 3: No solution\\n');\nrankA3 = rank(A3);\nfprintf('Rank of A3: %d\\n', rankA3);\n\n% Solving with pseudo inverse\nx_pseudo3 = pinv(A3) * b;\nfprintf('Pseudo inverse solution for A3 (least squares):\\n');\ndisp(x_pseudo3);\n\n% Since A3 is inconsistent, there is no exact solution, but the pseudo\n% inverse gives a least-squares solution.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Creating a matrix A which give a solution, $Ax=b$!"
    ]
  },
  {
    "objectID": "2024/weeks/week02/five_decomposition.html",
    "href": "2024/weeks/week02/five_decomposition.html",
    "title": "Five Fundamental Decompositions in Linear Algebra (Adapted from Gilbert Strang‚Äôs Approach)",
    "section": "",
    "text": "Introduction\nLinear algebra provides various tools for decomposing matrices, each with unique applications in engineering, data science, and machine learning. Here we discuss Five Fundamental Matrix Decompositions: CR decomposition, LU decomposition, Spectral Decomposition, QR decomposition, and Singular Value Decomposition (SVD). These decompositions help us understand matrix structure, solve linear systems, and analyze large datasets.\n\n\n\n1. CR Decomposition (Column-Row Decomposition)\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The CR decomposition expresses a matrix \\(A\\) as the product of two matrices‚Äîone made up of its columns and the other of its rows:\n\\[ A = C R \\]\n\n\nGiven a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), the CR decomposition is written as:\n\\[ A = C R \\]\nwhere: - \\(C \\in \\mathbb{R}^{m \\times k}\\) consists of \\(k\\) linearly independent columns of \\(A\\), - \\(R \\in \\mathbb{R}^{k \\times n}\\) contains corresponding rows that, when multiplied by \\(C\\), reconstruct \\(A\\).\n\nExample\nLet‚Äôs take an example where \\(A\\) is given by:\n\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 9 \\\\ 7 & 8 & 15 \\end{bmatrix} \\]\nWe can choose the first two columns of \\(A\\) to form the matrix \\(C\\) as they are independent:\n\\[ C = \\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\\\ 7 & 8 \\end{bmatrix} \\]\nNow, find \\(R\\), the row reduced echelon form of \\(A\\):\n\\[ R = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\]\nThus, the CR decomposition is, \\(A=CR\\):\n\\[ A = \\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\\\ 7 & 8 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\]\n\n\n\n\n\n\nNote\n\n\n\nApplication in Data Science: CR decomposition is useful for dimensionality reduction by approximating a matrix with lower rank, simplifying large datasets.\n\n\n\n\n\n\n2. LU Decomposition\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The LU decomposition factors a matrix \\(A\\) into a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\):\n\\[ A = LU \\]\n\n\nThe decomposition exists for square matrices \\(A\\) and can be extended to non-square matrices using pivoting techniques.\nLU decomposition is computed through Gaussian elimination. The matrix \\(A\\) is reduced to an upper triangular matrix, and the operations applied are stored in \\(L\\).\n\nExample\nConsider the matrix:\n\\[ A = \\begin{bmatrix} 4 & 3 \\\\ 6 & 3 \\end{bmatrix} \\]\nWe decompose \\(A\\) into:\n\\[ L = \\begin{bmatrix} 1 & 0 \\\\ 1.5 & 1 \\end{bmatrix}, \\quad U = \\begin{bmatrix} 4 & 3 \\\\ 0 & -1.5 \\end{bmatrix} \\]\nThus, the LU decomposition is:\n\\[ A = \\begin{bmatrix} 1 & 0 \\\\ 1.5 & 1 \\end{bmatrix} \\begin{bmatrix} 4 & 3 \\\\ 0 & -1.5 \\end{bmatrix} \\]\n\n\n\n\n\n\nNote\n\n\n\nApplication in Engineering: LU decomposition is used for efficiently solving systems of linear equations by forward and backward substitution.\n\n\n\n\n\n\n3. Spectral Decomposition\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The Spectral Decomposition applies to symmetric matrices and expresses \\(A\\) as:\n\\[ A = Q \\Lambda Q^T \\]\n\n\nWhere: - \\(Q\\) is an orthogonal matrix with eigenvectors of \\(A\\), - \\(\\Lambda\\) is a diagonal matrix with eigenvalues of \\(A\\).\nFor any symmetric matrix \\(A\\), its eigenvectors form an orthogonal basis, and \\(A\\) can be diagonalized as:\n\\[ A = Q \\Lambda Q^T \\]\n\nExample\nLet:\n\\[ A = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\]\nThe eigenvalues are \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = 2\\), and the eigenvectors are:\n\\[ v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad v_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\]\nThus:\n\\[ Q = \\begin{bmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\\\ 1/\\sqrt{2} & 1/\\sqrt{2} \\end{bmatrix}, \\quad \\Lambda = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix} \\]\nSo, the spectral decomposition is:\n\\[ A = Q \\Lambda Q^T \\]\n\n\n\n\n\n\nNote\n\n\n\nApplication in PCA: Spectral decomposition is widely used in Principal Component Analysis (PCA), where data is projected onto principal components to reduce dimensionality.\n\n\n\n\n\n\n4. QR Decomposition\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The QR decomposition factors a matrix \\(A\\) into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\):\n\\[ A = QR \\]\n\n\nQR decomposition is computed using the Gram-Schmidt process, which orthogonalizes the columns of \\(A\\) to form \\(Q\\), and the resulting triangular factors are stored in \\(R\\).\n\nExample\nGiven the matrix:\n\\[ A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\]\nUsing Gram-Schmidt, we obtain:\n\\[ Q = \\begin{bmatrix} 0.17 & 0.83 \\\\ 0.50 & 0.12 \\\\ 0.83 & -0.54 \\end{bmatrix}, \\quad R = \\begin{bmatrix} 6.08 & 7.44 \\\\ 0 & 0.83 \\end{bmatrix} \\]\nThus:\n\\[ A = QR \\]\n\n\n\n\n\n\nNote\n\n\n\nApplication in Machine Learning: QR decomposition is used in linear regression for finding the least-squares solution by solving overdetermined systems.\n\n\n\n\n\n\n5. Singular Value Decomposition (SVD)\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The Singular Value Decomposition (SVD) of any matrix \\(A\\) expresses it as:\n\\[ A = U \\Sigma V^T \\]\n\n\nWhere: - \\(U\\) contains left singular vectors, - \\(\\Sigma\\) is a diagonal matrix with singular values, - \\(V\\) contains right singular vectors.\nSVD generalizes the eigendecomposition to non-square matrices. Every matrix \\(A\\) can be factored as \\(A = U \\Sigma V^T\\).\n\nExample\nLet:\n\\[ A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\]\nThe singular values are \\(\\sigma_1 = 4\\) and \\(\\sigma_2 = 2\\). The singular vectors form \\(U\\) and \\(V\\):\n\\[ U = \\begin{bmatrix} 0.71 & -0.71 \\\\ 0.71 & 0.71 \\end{bmatrix}, \\quad \\Sigma = \\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix}, \\quad V = \\begin{bmatrix} 0.71 & -0.71 \\\\ 0.71 & 0.71 \\end{bmatrix} \\]\nThus, the SVD is:\n\\[ A = U \\Sigma V^T \\]\n\n\n\n\n\n\nNote\n\n\n\nApplications in Recommender Systems: SVD is widely used in collaborative filtering algorithms for matrix factorization in recommender systems.\n\n\n\n\n\n\nComparative Study of the Decompositions\n\n\n\n\n\n\n\n\n\nDecomposition\nForm\nProperties\nUse Cases\n\n\n\n\nCR Decomposition\n\\(A = CR\\)\nLow-rank approximation\nDimensionality reduction in Data Science\n\n\nLU Decomposition\n\\(A = LU\\)\nEfficient solving of linear systems\nEngineering, linear equations\n\n\nSpectral Decomposition\n\\(A = Q \\Lambda Q^T\\)\nSymmetric matrices\nPrincipal Component Analysis (PCA), vibration analysis\n\n\nQR Decomposition\n\\(A = QR\\)\nOrthogonal matrix, least squares\nEigenvalue problems, optimization, linear regression\n\n\nSingular Value Decomposition (SVD)\n\\(A = U \\Sigma V^T\\)\nAny matrix, best low-rank approximation\nRecommender systems, text mining\n\n\n\nEach decomposition has a unique set of properties, making them powerful tools in solving problems across engineering, data science, and machine learning.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Five Fundamental Decompositions in Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html",
    "href": "2024/weeks/week02/class_work1.html",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "",
    "text": "Instructions: Solve the problems given in the course resource page. Some of the solutions are wrong. Identify the mistakes in the solution and correct it in your work.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-1",
    "href": "2024/weeks/week02/class_work1.html#problem-1",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 1",
    "text": "Problem 1\nFind the eigenvalues and eigenvectors of the matrix: \\[\nA = \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix}\n\\]\n\nSolution 1\n\nCharacteristic Polynomial: \\[\n\\text{det}(A - \\lambda I) = \\begin{vmatrix} 1 - \\lambda & 2 \\\\ 2 & 3 - \\lambda \\end{vmatrix} = (1 - \\lambda)(3 - \\lambda) - 4 = \\lambda^2 - 4\\lambda - 1 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 2 + \\sqrt{5}, \\lambda_2 = 2 - \\sqrt{5}\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 2 + \\sqrt{5}\\): \\[\n(A - (2 + \\sqrt{5})I)v = 0 \\implies \\begin{pmatrix} -1 - \\sqrt{5} & 2 \\\\ 2 & 1 - \\sqrt{5} \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 2 \\\\ 1 + \\sqrt{5} \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 2 - \\sqrt{5}\\): \\[\n(A - (2 - \\sqrt{5})I)v = 0 \\implies \\begin{pmatrix} -1 + \\sqrt{5} & 2 \\\\ 2 & 1 + \\sqrt{5} \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} 2 \\\\ 1 - \\sqrt{5} \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-2",
    "href": "2024/weeks/week02/class_work1.html#problem-2",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 2",
    "text": "Problem 2\nFind the eigenvalues and eigenvectors of the matrix: \\[\nB = \\begin{pmatrix} 4 & 1 \\\\ 2 & 3 \\end{pmatrix}\n\\]\n\nSolution 2\n\nCharacteristic Polynomial: \\[\n\\text{det}(B - \\lambda I) = \\begin{vmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda \\end{vmatrix} = (4 - \\lambda)(3 - \\lambda) - 2 = \\lambda^2 - 7\\lambda + 10 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 5, \\lambda_2 = 2\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 5\\): \\[\n(B - 5I)v = 0 \\implies \\begin{pmatrix} -1 & 1 \\\\ 2 & -2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 2\\): \\[\n(B - 2I)v = 0 \\implies \\begin{pmatrix} 2 & 1 \\\\ 2 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-3",
    "href": "2024/weeks/week02/class_work1.html#problem-3",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 3",
    "text": "Problem 3\nFind the eigenvalues and eigenvectors of the matrix: \\[\nC = \\begin{pmatrix} 3 & 5 \\\\ 2 & 4 \\end{pmatrix}\n\\]\n\nSolution 3\n\nCharacteristic Polynomial: \\[\n\\text{det}(C - \\lambda I) = \\begin{vmatrix} 3 - \\lambda & 5 \\\\ 2 & 4 - \\lambda \\end{vmatrix} = (3 - \\lambda)(4 - \\lambda) - 10 = \\lambda^2 - 7\\lambda + 2 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 6, \\lambda_2 = 1\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 6\\): \\[\n(C - 6I)v = 0 \\implies \\begin{pmatrix} -3 & 5 \\\\ 2 & -2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 1\\): \\[\n(C - 1I)v = 0 \\implies \\begin{pmatrix} 2 & 5 \\\\ 2 & 3 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} -5 \\\\ 2 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-4",
    "href": "2024/weeks/week02/class_work1.html#problem-4",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 4",
    "text": "Problem 4\nFind the eigenvalues and eigenvectors of the matrix: \\[\nD = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n\\]\n\nSolution 4\n\nCharacteristic Polynomial: \\[\n\\text{det}(D - \\lambda I) = \\begin{vmatrix} -\\lambda & 1 \\\\ 0 & -\\lambda \\end{vmatrix} = \\lambda^2 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 0\\) (with algebraic multiplicity 2)\nEigenvectors:\n\nFor \\(\\lambda_1 = 0\\): \\[\n(D - 0I)v = 0 \\implies \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-5",
    "href": "2024/weeks/week02/class_work1.html#problem-5",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 5",
    "text": "Problem 5\nFind the eigenvalues and eigenvectors of the matrix: \\[\nE = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}\n\\]\n\nSolution 5\n\nCharacteristic Polynomial: \\[\n\\text{det}(E - \\lambda I) = \\begin{vmatrix} 2 - \\lambda & 0 \\\\ 0 & 3 - \\lambda \\end{vmatrix} = (2 - \\lambda)(3 - \\lambda) = 0\n\\] Eigenvalues: \\(\\lambda_1 = 2, \\lambda_2 = 3\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 2\\): \\[\n(E - 2I)v = 0 \\implies \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 3\\): \\[\n(E - 3I)v = 0 \\implies \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-6",
    "href": "2024/weeks/week02/class_work1.html#problem-6",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 6",
    "text": "Problem 6\nFind the eigenvalues and eigenvectors of the matrix: \\[\nF = \\begin{pmatrix} 4 & 1 \\\\ 2 & 5 \\end{pmatrix}\n\\]\n\nSolution 6\n\nCharacteristic Polynomial: \\[\n\\text{det}(F - \\lambda I) = \\begin{vmatrix} 4 - \\lambda & 1 \\\\ 2 & 5 - \\lambda \\end{vmatrix} = (4 - \\lambda)(5 - \\lambda) - 2 = \\lambda^2 - 9\\lambda + 18 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 6, \\lambda_2 = 3\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 6\\): \\[\n(F - 6I)v = 0 \\implies \\begin{pmatrix} -2 & 1 \\\\ 2 & -1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 3\\): \\[\n(F - 3I)v = 0 \\implies \\begin{pmatrix} 1 & 1 \\\\ 2 & 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-7",
    "href": "2024/weeks/week02/class_work1.html#problem-7",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 7",
    "text": "Problem 7\nFind the eigenvalues and eigenvectors of the matrix: \\[\nG = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}\n\\]\n\nSolution 7\n\nCharacteristic Polynomial: \\[\n\\text{det}(G - \\lambda I) = \\begin{vmatrix} 1 - \\lambda & 2 \\\\ 2 & 1 - \\lambda \\end{vmatrix} = (1 - \\lambda)^2 - 4 = \\lambda^2 - 2\\lambda - 3 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 3, \\lambda_2 = -1\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 3\\): \\[\n(G - 3I)v = 0 \\implies \\begin{pmatrix} -2 & 2 \\\\ 2 & -2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = -1\\): \\[\n(G + I)v = 0 \\implies \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-8",
    "href": "2024/weeks/week02/class_work1.html#problem-8",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 8",
    "text": "Problem 8\nFind the eigenvalues and eigenvectors of the matrix: \\[\nH = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\n\\]\n\nSolution 8\n\nCharacteristic Polynomial: \\[\n\\text{det}(H - \\lambda I) = \\begin{vmatrix} 2 - \\lambda & 3 \\\\ 2 & 1 - \\lambda \\end{vmatrix} = (2 - \\lambda)(1 - \\lambda) - 6 = \\lambda^2 - 3\\lambda - 4 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 4, \\lambda_2 = -1\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 4\\): \\[\n(H - 4I)v = 0 \\implies \\begin{pmatrix} -2 & 3 \\\\ 2 & -3 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = -1\\): \\[\n(H + I)v = 0 \\implies \\begin{pmatrix} 3 & 3 \\\\ 2 & 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-9",
    "href": "2024/weeks/week02/class_work1.html#problem-9",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 9",
    "text": "Problem 9\nFind the eigenvalues and eigenvectors of the matrix: \\[\nI = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n\\]\n\nSolution 9\n\nCharacteristic Polynomial: \\[\n\\text{det}(I - \\lambda I) = \\begin{vmatrix} 1 - \\lambda & 0 \\\\ 0 & 1 - \\lambda \\end{vmatrix} = (1 - \\lambda)^2 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 1\\) (with algebraic multiplicity 2)\nEigenvectors:\n\nFor \\(\\lambda_1 = 1\\): \\[\n(I - I)v = 0 \\implies \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-1-1",
    "href": "2024/weeks/week02/class_work1.html#problem-1-1",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 1",
    "text": "Problem 1\nFind the eigenvalues and eigenvectors of the matrix: \\[\nA = \\begin{pmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}\n\\]\n\nSolution 1\n\nCharacteristic Polynomial: \\[\n\\text{det}(A - \\lambda I) = \\begin{vmatrix} 2 - \\lambda & 1 & 0 \\\\ 1 & 2 - \\lambda & 1 \\\\ 0 & 1 & 2 - \\lambda \\end{vmatrix} = (2 - \\lambda)^3 - 2 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 3, \\lambda_2 = 1\\) (with algebraic multiplicity 1)\nEigenvectors:\n\nFor \\(\\lambda_1 = 3\\): \\[\n(A - 3I)v = 0 \\implies \\begin{pmatrix} -1 & 1 & 0 \\\\ 1 & -1 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 1\\): \\[\n(A - I)v = 0 \\implies \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-2-1",
    "href": "2024/weeks/week02/class_work1.html#problem-2-1",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 2",
    "text": "Problem 2\nFind the eigenvalues and eigenvectors of the matrix: \\[\nB = \\begin{pmatrix} 4 & 1 & 2 \\\\ 1 & 4 & 3 \\\\ 2 & 3 & 4 \\end{pmatrix}\n\\]\n\nSolution 2\n\nCharacteristic Polynomial: \\[\n\\text{det}(B - \\lambda I) = \\begin{vmatrix} 4 - \\lambda & 1 & 2 \\\\ 1 & 4 - \\lambda & 3 \\\\ 2 & 3 & 4 - \\lambda \\end{vmatrix} = (4 - \\lambda)^3 - 6(4 - \\lambda) = 0\n\\] Eigenvalues: \\(\\lambda_1 = 6, \\lambda_2 = 3\\) (with algebraic multiplicity 1)\nEigenvectors:\n\nFor \\(\\lambda_1 = 6\\): \\[\n(B - 6I)v = 0 \\implies \\begin{pmatrix} -2 & 1 & 2 \\\\ 1 & -2 & 3 \\\\ 2 & 3 & -2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 3\\): \\[\n(B - 3I)v = 0 \\implies \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 3 \\\\ 2 & 3 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-3-1",
    "href": "2024/weeks/week02/class_work1.html#problem-3-1",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 3",
    "text": "Problem 3\nFind the eigenvalues and eigenvectors of the matrix: \\[\nC = \\begin{pmatrix} 5 & 4 & 2 \\\\ 4 & 6 & 3 \\\\ 2 & 3 & 4 \\end{pmatrix}\n\\]\n\nSolution 3\n\nCharacteristic Polynomial: \\[\n\\text{det}(C - \\lambda I) = \\begin{vmatrix} 5 - \\lambda & 4 & 2 \\\\ 4 & 6 - \\lambda & 3 \\\\ 2 & 3 & 4 - \\lambda \\end{vmatrix} = (5 - \\lambda)((6 - \\lambda)(4 - \\lambda) - 9) - 4(4(4 - \\lambda) - 6) + 2(12 - 3(6 - \\lambda))\n\\] Solving the determinant gives the eigenvalues: \\(\\lambda_1 = 12, \\lambda_2 = 3, \\lambda_3 = 0\\)\nEigenvectors:\n\nFor \\(\\lambda_1 = 12\\): \\[\n(C - 12I)v = 0 \\implies \\begin{pmatrix} -7 & 4 & 2 \\\\ 4 & -6 & 3 \\\\ 2 & 3 & -8 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}\n\\]\nFor \\(\\lambda_2 = 3\\): \\[\n(C - 3I)v = 0 \\implies \\begin{pmatrix} 2 & 4 & 2 \\\\ 4 & 3 & 3 \\\\ 2 & 3 & 1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_2 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}\n\\]\nFor \\(\\lambda_3 = 0\\): \\[\n(C - 0I)v = 0 \\implies \\begin{pmatrix} 5 & 4 & 2 \\\\ 4 & 6 & 3 \\\\ 2 & 3 & 4 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_3 = \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-4-1",
    "href": "2024/weeks/week02/class_work1.html#problem-4-1",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 4",
    "text": "Problem 4\nFind the eigenvalues and eigenvectors of the matrix: \\[\nD = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix}\n\\]\n\nSolution 4\n\nCharacteristic Polynomial: \\[\n\\text{det}(D - \\lambda I) = \\begin{vmatrix} 2 - \\lambda & 0 & 0 \\\\ 0 & 2 - \\lambda & 1 \\\\ 0 & 0 & 2 - \\lambda \\end{vmatrix} = (2 - \\lambda)^3 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 2\\) (with algebraic multiplicity 3)\nEigenvectors:\n\nFor \\(\\lambda_1 = 2\\): \\[\n(D - 2I)v = 0 \\implies \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, v_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#problem-5-1",
    "href": "2024/weeks/week02/class_work1.html#problem-5-1",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Problem 5",
    "text": "Problem 5\nFind the eigenvalues and eigenvectors of the matrix: \\[\nE = \\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 3 \\end{pmatrix}\n\\]\n\nSolution 5\n\nCharacteristic Polynomial: \\[\n\\text{det}(E - \\lambda I) = \\begin{vmatrix} 3 - \\lambda & 0 & 0 \\\\ 0 & 3 - \\lambda & 1 \\\\ 0 & 0 & 3 - \\lambda \\end{vmatrix} = (3 - \\lambda)^3 = 0\n\\] Eigenvalues: \\(\\lambda_1 = 3\\) (with algebraic multiplicity 3)\nEigenvectors:\n\nFor \\(\\lambda_1 = 3\\): \\[\n(E - 3I)v = 0 \\implies \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 0 \\implies v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, v_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week02/class_work1.html#diagonalization-of-a-matrix",
    "href": "2024/weeks/week02/class_work1.html#diagonalization-of-a-matrix",
    "title": "Eigenvalues and Eigenvectors Problems",
    "section": "Diagonalization of a matrix",
    "text": "Diagonalization of a matrix\nLet \\(A\\) be square matrix, with all eigen values satisfies the relation \\(A.M=G.M\\), then the diagonal form of \\(A\\) is \\[D=B^{-1}DB\\]\nwhere \\(D\\) is the diagonal matrix of eigen values of \\(A\\), \\(B\\) is the modal matrix, which contains eigen values of as columns.\n\n\n\n\n\n\nWhen \\(A\\) is diagonalizable\n\n\n\nThe matrx \\(A\\) is diagonalizable only if Algebraic Multiplicity (A.M) and Geometric Multiplicity (G.M) of each eigen value \\(\\lambda\\) of \\(A\\) are same.\nAlgebraic Multiplicity: Number of times the eigen value \\(\\lambda\\) repeats in the solution of the characteristic polynomial of \\(A\\).\nGeometric Multiplicity: Number of vectors in the eigen space of \\(\\lambda\\).\n\n\n\nProblems\n\nDiagonalize the following matrices if possible.\n\n\n\\(A=\\begin{bmatrix} 3&-2&0\\\\ -2&3&0\\\\ 0&0&5\\end{bmatrix}\\).\n\\(A=\\begin{bmatrix}2&1&-1\\\\ 1&1&-2\\\\ -1&-2&1\\end{bmatrix}\\).\n\\(A=\\begin{bmatrix}-2& 2&-3\\\\ 2&1&-6\\\\ -1&-2&0\\end{bmatrix}\\).\n\\(A=\\begin{bmatrix} 2&1&0\\\\ 0&1&-1\\\\ 0&2&4\\end{bmatrix}\\).\n\\(A=\\begin{bmatrix}-3&-7&-5\\\\ 2&4&3\\\\ 1&2&2\\end{bmatrix}\\).\nIf \\(2\\) is an eigen value of \\(\\begin{bmatrix}3 &-1&1\\\\ -1&5&-1\\\\ 1&-1&3\\end{bmatrix}\\), without using its characteristic polynomial, find the other eigen values. Also find the eigen values of \\(A^3,A^T,A^{-1}, 5A, A-3I\\) and \\(\\text{adj}(A)\\).\nExamine whether \\(A=\\begin{bmatrix}7&-1\\\\4 &3\\end{bmatrix}\\) is diagonalizable or not.\nDiagonalize \\(A=\\begin{bmatrix}2&0&1\\\\ 0&2&0\\\\ 1&0&2\\end{bmatrix}\\).\nExamine whether the matrix \\(A=\\begin{bmatrix}1&-3&3\\\\ 0&-5&6\\\\ 0&-3&4\\end{bmatrix}\\) is diagonalizable. If yes find the diagonal form.\nExamine whether \\(A=\\begin{bmatrix}1&2&2\\\\ 0&2&1\\\\ -1&2&2\\end{bmatrix}\\) is diagonalizable or not. If so diagonalize it.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Class Work Resource"
    ]
  },
  {
    "objectID": "2024/weeks/week01/slides.html#your-professor",
    "href": "2024/weeks/week01/slides.html#your-professor",
    "title": "üóìÔ∏è Week 01 Introduction",
    "section": "Your Professor",
    "text": "Your Professor\n\n\n\n\n\nProf.¬†(Dr.) Soman K.P. Professor & Dean Amrita School of Artificial Intelligence\n\n\n\n\n\nPh.D., IIT-Kharagpur\nM.Tech. in Reliability Engineering, IIT-Kharagpur\nPost Master Diploma in Statistical Quality Control and Operations Research, Indian Statistical Institute, Calcutta\nBSc Engg. in Electrical Engineering, REC (Now NIT) Kozhikode\n\nDean ASAI  Professor, Electronics and Communication Engineering  World‚Äôs Top 2% Most Influential Scientist"
  },
  {
    "objectID": "2024/weeks/week01/slides.html#course-content-developers",
    "href": "2024/weeks/week01/slides.html#course-content-developers",
    "title": "üóìÔ∏è Week 01 Introduction",
    "section": "Course Content Developers",
    "text": "Course Content Developers\n\n\n\n\n\nAssistant Professor  PhD in Image Processing  M.Tech. in Artificial Intelligence üìß \n\n\n\n\n\n\nResearch Scholor  PhD Candidate at ASAI  MSc in Mathematics üìß \n\n\n\n\n\n\n\n24MA602 ‚ÄìComputational Mathematics for Data Science"
  },
  {
    "objectID": "2024/weeks/week01/linear_transformations.html",
    "href": "2024/weeks/week01/linear_transformations.html",
    "title": "Linear Transformations: The Essence of Linearity",
    "section": "",
    "text": "Linear transformations lie at the heart of linear algebra. They map vectors between vector spaces while preserving the structure of vector addition and scalar multiplication. In this section, the concept of linear transformations is introduced as a way to extend our understanding of matrix operations into more abstract and general contexts.\n\nDefinition: Mapping Vectors While Preserving Linearity\nA linear transformation, denoted by \\(T\\), is a mapping between two vector spaces, \\(V\\) (input space) and \\(Y\\) (output space), that adheres to the principle of linearity:\n\\[\nT(c\\mathbf{v} + d\\mathbf{w}) = cT(\\mathbf{v}) + dT(\\mathbf{w})\n\\]\nThis equation encapsulates the essence of linearity:\n\nAdditivity: Transforming the sum of two vectors is equivalent to transforming each vector individually and then adding the transformed results.\nHomogeneity: Scaling a vector by a constant and then transforming it is the same as transforming the vector first and then scaling the result.\n\nExamples of linear transformations include rotations in the x-y plane, and the derivative operation in function spaces.\n\n\nMatrices as Representations: A Basis-Dependent Encoding\nLinear transformations can be represented by matrices, though this representation depends on the choice of bases for the vector spaces involved. Here‚Äôs how to construct the matrix \\(A\\) that represents a linear transformation \\(T\\):\n\nChoose bases: Select a basis \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}\\) for the input space \\(V\\), and a basis \\(\\{\\mathbf{y}_1, \\mathbf{y}_2, \\dots, \\mathbf{y}_m\\}\\) for the output space \\(Y\\).\nTransform basis vectors: Apply the transformation \\(T\\) to each input basis vector \\(\\mathbf{v}_j\\).\nExpress transformed vectors in the output basis: Write each transformed vector \\(T(\\mathbf{v}_j)\\) as a linear combination of the output basis vectors:\n\n\\[\nT(\\mathbf{v}_j) = a_{1j} \\mathbf{y}_1 + a_{2j} \\mathbf{y}_2 + \\dots + a_{mj} \\mathbf{y}_m\n\\]\n\nForm the matrix: The coefficients \\(a_{ij}\\) form the entries of the matrix \\(A\\), where the \\(j\\)-th column of \\(A\\) corresponds to the transformation of the \\(j\\)-th input basis vector.\n\nThis matrix \\(A\\) encodes the action of the linear transformation \\(T\\) on any vector expressed in the chosen input basis. When \\(A\\) is applied to a coordinate vector, it returns the transformed vector in terms of the output basis.\n\n\nExample: Rotation in the Plane\nConsider a rotation by an angle \\(\\theta\\) in the plane. The corresponding transformation matrix \\(A_\\theta\\) is given by:\n\\[\nA_\\theta =\n\\begin{pmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{pmatrix}\n\\]\nThis matrix represents the linear transformation that rotates a vector in \\(\\mathbb{R}^2\\) by an angle \\(\\theta\\).\n\n\nBasis Changes: Transforming the Matrix Representation\nThe matrix representation of a linear transformation depends on the choice of bases for both the input and output spaces. Changing these bases alters the matrix representation. Let \\(M\\) represent \\(T\\) with respect to new bases \\(\\{V_1, V_2, \\dots, V_n\\}\\) in \\(V\\) and \\(\\{Y_1, Y_2, \\dots, Y_m\\}\\) in \\(Y\\). The new matrix \\(M\\) is related to the old matrix \\(A\\) by:\n\\[\nM = Y^{-1} A V\n\\]\nwhere:\n\n\\(V\\) is the change-of-basis matrix for the input space, and\n\\(Y\\) is the change-of-basis matrix for the output space.\n\n\n\nExample: Scaling and Shearing Transformation\nConsider a linear transformation \\(T\\) that scales a vector by a factor of 2 in the \\(x\\)-direction and shears it in the \\(y\\)-direction. The corresponding matrix in standard basis is:\n\\[\nA =\n\\begin{pmatrix}\n2 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\nIf we change the basis of the input space using a matrix \\(V\\) and of the output space using a matrix \\(Y\\), the transformation matrix changes accordingly.\n\n\nSignificance: Unifying Framework for Linearity\nLinear transformations provide a unified framework for understanding key concepts in linear algebra. The matrix representation of a linear transformation allows us to interpret matrix operations in terms of transformations:\n\nMatrix multiplication as composition of transformations: Multiplying two matrices corresponds to composing the linear transformations they represent.\nInvertible matrices as invertible transformations: A matrix is invertible if and only if the corresponding linear transformation is invertible.\nEigenvectors as invariant directions: Eigenvectors of a matrix represent directions in the input space that remain unchanged (up to scaling) under the corresponding linear transformation.\n\n\n\nApplications of Linear Transformations\n\n1. Computer Graphics and Animations\nLinear transformations are fundamental in computer graphics, where they are used to rotate, scale, and translate objects in 2D and 3D spaces. The transformation of an object is often represented by multiplying the vector of points defining the object by a transformation matrix.\n\n\n2. Differential Equations\nIn solving systems of linear differential equations, linear transformations are used to diagonalize the system‚Äôs matrix and simplify the solution process. This allows for easier integration of the system by working with independent variables.\n\n\n3. Data Analysis and Machine Learning\nIn machine learning, linear transformations form the basis for techniques like Principal Component Analysis (PCA), which transforms data into a new basis defined by its principal components. This reduces dimensionality while preserving the most significant features of the data.\n\n\n4. Control Systems\nIn control theory, linear transformations are used to model and analyze dynamic systems. They simplify complex systems by transforming them into more manageable representations, which can be used to predict and control system behavior.\n\n\n\n\n\n\nKey Concepts:\n\n\n\n\nLinear Transformation: A mapping between two vector spaces that preserves vector addition and scalar multiplication.\nMatrix Representation: Every linear transformation can be represented by a matrix, but this depends on the choice of input and output bases.\nChange of Basis: Changing the basis of either the input or output space transforms the matrix representation of the linear transformation.\nApplications: Linear transformations are used in graphics, differential equations, machine learning, and control systems.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Linear Transformations- The Essence of Linearity"
    ]
  },
  {
    "objectID": "2024/weeks/week01/algebra_geometry.html",
    "href": "2024/weeks/week01/algebra_geometry.html",
    "title": "Unifying Coordinate Geometry with Linear Algebra",
    "section": "",
    "text": "In this discussion, we will bridge the gap between coordinate geometry and linear algebra by exploring how geometric objects such as lines and planes can be represented algebraically. This journey begins with familiar notions from high school geometry, such as lines and planes in 3D, and leads us into the powerful abstractions provided by linear algebra, such as vector spaces, subspaces, and matrices.\nWe aim to answer the question: Which is better for representing subspaces‚Äîbasis sets or linear equations? We will use examples from both geometry and algebra to build a unified understanding.\n\n\n\nIn three-dimensional space, a line can be represented parametrically using a point \\(P_0(x_0, y_0, z_0)\\) and a direction vector \\(\\mathbf{d} = (a, b, c)\\). The parametric equations for the line can be written as:\n\\[\n\\begin{aligned}\nx &= x_0 + at, \\\\\ny &= y_0 + bt, \\\\\nz &= z_0 + ct,\n\\end{aligned}\n\\]\nwhere \\(t\\) is a parameter.\nAlternatively, we can express a line using directional cosines, which are the cosines of the angles that the line makes with the coordinate axes. If the direction cosines are denoted as \\(l, m, n\\), where:\n\\[\nl = \\cos(\\alpha), \\quad m = \\cos(\\beta), \\quad n = \\cos(\\gamma),\n\\]\nthe equations can be expressed as:\n\\[\n\\begin{aligned}\nx &= x_0 + k \\cdot l, \\\\\ny &= y_0 + k \\cdot m, \\\\\nz &= z_0 + k \\cdot n,\n\\end{aligned}\n\\]\nwhere \\(k\\) is a scalar. This leads to the relationship:\n\\[\n\\frac{x - x_0}{l} = \\frac{y - y_0}{m} = \\frac{z - z_0}{n},\n\\]\nwhich can be simplified into a set of two independent simultaneous linear equations, demonstrating a deeper connection between the geometric representation and linear algebraic formulation.\n\n\nThe equation of a plane can be defined in multiple ways, but one common representation is through the normal vector and a point on the plane. If we have a point \\(P_0(x_0, y_0, z_0)\\) on the plane and a normal vector \\(\\mathbf{n} = (a, b, c)\\), the equation of the plane can be expressed as:\n\\[\na(x - x_0) + b(y - y_0) + c(z - z_0) = 0.\n\\]\nInterestingly, if we are given three non-collinear points \\(P_1(x_1, y_1, z_1)\\), \\(P_2(x_2, y_2, z_2)\\), and \\(P_3(x_3, y_3, z_3)\\), we can also find the equation of the plane. The vectors \\(\\mathbf{v_1} = P_2 - P_1\\) and \\(\\mathbf{v_2} = P_3 - P_1\\) are formed, and their cross product \\(\\mathbf{n} = \\mathbf{v_1} \\times \\mathbf{v_2}\\) gives us the normal vector to the plane. Using the normal vector and one of the points, we can derive the plane equation.\n\n\n\n\nIn linear algebra, we discuss concepts such as vector spaces, linear combinations, subspaces, and bases. A vector space is a collection of vectors that can be added together and multiplied by scalars. A subspace is a subset of a vector space that is itself a vector space under the same operations.\nA basis of a vector space is a set of vectors that are linearly independent and span the space. For example, in \\(\\mathbb{R}^3\\), the standard basis consists of the vectors \\(\\mathbf{e_1} = (1,0,0)\\), \\(\\mathbf{e_2} = (0,1,0)\\), and \\(\\mathbf{e_3} = (0,0,1)\\).\nThe relationship between linear equations and subspaces becomes evident when considering the null space of a matrix. The null space consists of all vectors \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = 0\\), representing a subspace of the vector space defined by \\(A\\).\n\n\nEigenvalues and eigenvectors play a significant role in linear algebra. Given a square matrix \\(A\\), an eigenvalue \\(\\lambda\\) is defined by the equation:\n\\[\n(A - \\lambda I)\\mathbf{x} = 0,\n\\]\nwhere \\(I\\) is the identity matrix and \\(\\mathbf{x}\\) is the corresponding eigenvector.\nA major observation in this context is that solving \\((A - \\lambda I)\\mathbf{x} = 0\\) results in finding a vector \\(\\mathbf{x}\\) that is orthogonal to the transformation defined by \\(A - \\lambda I\\). Furthermore, since \\(A - \\lambda I\\) contains only two independent rows, the eigenvector \\(\\mathbf{x}\\) can be interpreted geometrically as the cross product of the independent vectors of \\(A - \\lambda I\\) in 3D space.\n\n\n\nUsing the concept of directional cosines, we can represent a line in 3D. By simplifying this representation, we can observe that it reduces to a system of two independent simultaneous linear equations. This further emphasizes the connection between geometric concepts and algebraic formulations.\n\n\n\n\nBy synthesizing concepts from coordinate geometry and linear algebra, we find a rich interplay between algebraic equations and geometric interpretations. We observe how algebraic manipulations of vectors, represented in matrices, connect with geometric representations in \\(\\mathbb{R}^3\\).\nThis unification reveals deeper mathematical structures, particularly the concept of matrices as versatile tools for data representation, operations, and abstract computations. Matrices allow us to encapsulate geometric transformations, linear relationships, and eigenvalue problems in a single mathematical framework. Their compatibility with computation and analysis enhances our ability to model, analyze, and solve real-world problems in multi-dimensional spaces.\nThe abstract nature of matrices helps bridge various mathematical concepts, leading to innovative applications across disciplines.\n\n\n\n\nFind the equation of the plane passing through three points: \\((1, 2, 0)\\), \\((3, 4, 5)\\), \\((6, 7, 8)\\).\n\nSolution:\nTo find the equation of the plane passing through the points \\(P_1(1, 2, 0)\\), \\(P_2(3, 4, 5)\\), and \\(P_3(6, 7, 8)\\), we start by forming two vectors from these points:\n\\[\n\\mathbf{v_1} = P_2 - P_1 = (3 - 1, 4 - 2, 5 - 0) = (2, 2, 5)\n\\]\n\\[\n\\mathbf{v_2} = P_3 - P_1 = (6 - 1, 7 - 2, 8 - 0) = (5, 5, 8)\n\\]\nNext, we find the normal vector \\(\\mathbf{n}\\) to the plane by taking the cross product of \\(\\mathbf{v_1}\\) and \\(\\mathbf{v_2}\\):\n\\[\n\\mathbf{n} = \\mathbf{v_1} \\times \\mathbf{v_2} =\n\\begin{vmatrix}\n\\hat{i} & \\hat{j} & \\hat{k} \\\\\n2 & 2 & 5 \\\\\n5 & 5 & 8\n\\end{vmatrix}\n\\]\nCalculating the determinant, we have:\n\\[\n\\mathbf{n} = \\hat{i}(2 \\cdot 8 - 5 \\cdot 5) - \\hat{j}(2 \\cdot 8 - 5 \\cdot 5) + \\hat{k}(2 \\cdot 5 - 2 \\cdot 5)\n\\]\nThis simplifies to:\n\\[\n\\mathbf{n} = \\hat{i}(-9) + \\hat{j}(9) + \\hat{k}(0) = (-9, 9, 0)\n\\]\nThe equation of the plane can be expressed as:\n\\[\nax + by + cz = d\n\\]\nwhere \\(\\mathbf{n} = (a, b, c) = (-9, 9, 0)\\). Using the point \\(P_1(1, 2, 0)\\) to find \\(d\\):\n\\[\n-9(1) + 9(2) + 0(0) = d\n\\]\nThis gives:\n\\[\n-9 + 18 = d \\implies d = 9\n\\]\nThus, the equation of the plane is:\n\\[\n-9x + 9y = 9 \\quad \\text{or} \\quad x - y = -1.\n\\]\n\nFind the directional cosines of the line passing through points \\((1, 1, 1)\\) and \\((4, 5, 6)\\).\n\nSolution:\nThe direction vector of the line is:\n\\[\n\\mathbf{d} = (4 - 1, 5 - 1, 6 - 1) = (3, 4, 5).\n\\]\nThe magnitude of the direction vector is:\n\\[\n|\\mathbf{d}| = \\sqrt{3^2 + 4^2 + 5^2} = \\sqrt{9 + 16 + 25} = \\sqrt{50}.\n\\]\nThe directional cosines are:\n\\[\nl = \\frac{3}{\\sqrt{50}}, \\quad m = \\frac{4}{\\sqrt{50}}, \\quad n = \\frac{5}{\\sqrt{50}}.\n\\]\nThus, the directional cosines are \\(l = \\frac{3}{\\sqrt{50}}\\), \\(m = \\frac{4}{\\sqrt{50}}\\), and \\(n = \\frac{5}{\\sqrt{50}}\\).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Week-1 Summary"
    ]
  },
  {
    "objectID": "2024/weeks/week01/algebra_geometry.html#introduction",
    "href": "2024/weeks/week01/algebra_geometry.html#introduction",
    "title": "Unifying Coordinate Geometry with Linear Algebra",
    "section": "",
    "text": "In this discussion, we will bridge the gap between coordinate geometry and linear algebra by exploring how geometric objects such as lines and planes can be represented algebraically. This journey begins with familiar notions from high school geometry, such as lines and planes in 3D, and leads us into the powerful abstractions provided by linear algebra, such as vector spaces, subspaces, and matrices.\nWe aim to answer the question: Which is better for representing subspaces‚Äîbasis sets or linear equations? We will use examples from both geometry and algebra to build a unified understanding.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Week-1 Summary"
    ]
  },
  {
    "objectID": "2024/weeks/week01/algebra_geometry.html#coordinate-geometry-lines-planes-and-directional-cosines",
    "href": "2024/weeks/week01/algebra_geometry.html#coordinate-geometry-lines-planes-and-directional-cosines",
    "title": "Unifying Coordinate Geometry with Linear Algebra",
    "section": "",
    "text": "In three-dimensional space, a line can be represented parametrically using a point \\(P_0(x_0, y_0, z_0)\\) and a direction vector \\(\\mathbf{d} = (a, b, c)\\). The parametric equations for the line can be written as:\n\\[\n\\begin{aligned}\nx &= x_0 + at, \\\\\ny &= y_0 + bt, \\\\\nz &= z_0 + ct,\n\\end{aligned}\n\\]\nwhere \\(t\\) is a parameter.\nAlternatively, we can express a line using directional cosines, which are the cosines of the angles that the line makes with the coordinate axes. If the direction cosines are denoted as \\(l, m, n\\), where:\n\\[\nl = \\cos(\\alpha), \\quad m = \\cos(\\beta), \\quad n = \\cos(\\gamma),\n\\]\nthe equations can be expressed as:\n\\[\n\\begin{aligned}\nx &= x_0 + k \\cdot l, \\\\\ny &= y_0 + k \\cdot m, \\\\\nz &= z_0 + k \\cdot n,\n\\end{aligned}\n\\]\nwhere \\(k\\) is a scalar. This leads to the relationship:\n\\[\n\\frac{x - x_0}{l} = \\frac{y - y_0}{m} = \\frac{z - z_0}{n},\n\\]\nwhich can be simplified into a set of two independent simultaneous linear equations, demonstrating a deeper connection between the geometric representation and linear algebraic formulation.\n\n\nThe equation of a plane can be defined in multiple ways, but one common representation is through the normal vector and a point on the plane. If we have a point \\(P_0(x_0, y_0, z_0)\\) on the plane and a normal vector \\(\\mathbf{n} = (a, b, c)\\), the equation of the plane can be expressed as:\n\\[\na(x - x_0) + b(y - y_0) + c(z - z_0) = 0.\n\\]\nInterestingly, if we are given three non-collinear points \\(P_1(x_1, y_1, z_1)\\), \\(P_2(x_2, y_2, z_2)\\), and \\(P_3(x_3, y_3, z_3)\\), we can also find the equation of the plane. The vectors \\(\\mathbf{v_1} = P_2 - P_1\\) and \\(\\mathbf{v_2} = P_3 - P_1\\) are formed, and their cross product \\(\\mathbf{n} = \\mathbf{v_1} \\times \\mathbf{v_2}\\) gives us the normal vector to the plane. Using the normal vector and one of the points, we can derive the plane equation.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Week-1 Summary"
    ]
  },
  {
    "objectID": "2024/weeks/week01/algebra_geometry.html#linear-algebra-vectors-subspaces-and-basis",
    "href": "2024/weeks/week01/algebra_geometry.html#linear-algebra-vectors-subspaces-and-basis",
    "title": "Unifying Coordinate Geometry with Linear Algebra",
    "section": "",
    "text": "In linear algebra, we discuss concepts such as vector spaces, linear combinations, subspaces, and bases. A vector space is a collection of vectors that can be added together and multiplied by scalars. A subspace is a subset of a vector space that is itself a vector space under the same operations.\nA basis of a vector space is a set of vectors that are linearly independent and span the space. For example, in \\(\\mathbb{R}^3\\), the standard basis consists of the vectors \\(\\mathbf{e_1} = (1,0,0)\\), \\(\\mathbf{e_2} = (0,1,0)\\), and \\(\\mathbf{e_3} = (0,0,1)\\).\nThe relationship between linear equations and subspaces becomes evident when considering the null space of a matrix. The null space consists of all vectors \\(\\mathbf{x}\\) such that \\(A\\mathbf{x} = 0\\), representing a subspace of the vector space defined by \\(A\\).\n\n\nEigenvalues and eigenvectors play a significant role in linear algebra. Given a square matrix \\(A\\), an eigenvalue \\(\\lambda\\) is defined by the equation:\n\\[\n(A - \\lambda I)\\mathbf{x} = 0,\n\\]\nwhere \\(I\\) is the identity matrix and \\(\\mathbf{x}\\) is the corresponding eigenvector.\nA major observation in this context is that solving \\((A - \\lambda I)\\mathbf{x} = 0\\) results in finding a vector \\(\\mathbf{x}\\) that is orthogonal to the transformation defined by \\(A - \\lambda I\\). Furthermore, since \\(A - \\lambda I\\) contains only two independent rows, the eigenvector \\(\\mathbf{x}\\) can be interpreted geometrically as the cross product of the independent vectors of \\(A - \\lambda I\\) in 3D space.\n\n\n\nUsing the concept of directional cosines, we can represent a line in 3D. By simplifying this representation, we can observe that it reduces to a system of two independent simultaneous linear equations. This further emphasizes the connection between geometric concepts and algebraic formulations.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Week-1 Summary"
    ]
  },
  {
    "objectID": "2024/weeks/week01/algebra_geometry.html#a-unified-view-of-coordinate-geometry-and-linear-algebra",
    "href": "2024/weeks/week01/algebra_geometry.html#a-unified-view-of-coordinate-geometry-and-linear-algebra",
    "title": "Unifying Coordinate Geometry with Linear Algebra",
    "section": "",
    "text": "By synthesizing concepts from coordinate geometry and linear algebra, we find a rich interplay between algebraic equations and geometric interpretations. We observe how algebraic manipulations of vectors, represented in matrices, connect with geometric representations in \\(\\mathbb{R}^3\\).\nThis unification reveals deeper mathematical structures, particularly the concept of matrices as versatile tools for data representation, operations, and abstract computations. Matrices allow us to encapsulate geometric transformations, linear relationships, and eigenvalue problems in a single mathematical framework. Their compatibility with computation and analysis enhances our ability to model, analyze, and solve real-world problems in multi-dimensional spaces.\nThe abstract nature of matrices helps bridge various mathematical concepts, leading to innovative applications across disciplines.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Week-1 Summary"
    ]
  },
  {
    "objectID": "2024/weeks/week01/algebra_geometry.html#tasks",
    "href": "2024/weeks/week01/algebra_geometry.html#tasks",
    "title": "Unifying Coordinate Geometry with Linear Algebra",
    "section": "",
    "text": "Find the equation of the plane passing through three points: \\((1, 2, 0)\\), \\((3, 4, 5)\\), \\((6, 7, 8)\\).\n\nSolution:\nTo find the equation of the plane passing through the points \\(P_1(1, 2, 0)\\), \\(P_2(3, 4, 5)\\), and \\(P_3(6, 7, 8)\\), we start by forming two vectors from these points:\n\\[\n\\mathbf{v_1} = P_2 - P_1 = (3 - 1, 4 - 2, 5 - 0) = (2, 2, 5)\n\\]\n\\[\n\\mathbf{v_2} = P_3 - P_1 = (6 - 1, 7 - 2, 8 - 0) = (5, 5, 8)\n\\]\nNext, we find the normal vector \\(\\mathbf{n}\\) to the plane by taking the cross product of \\(\\mathbf{v_1}\\) and \\(\\mathbf{v_2}\\):\n\\[\n\\mathbf{n} = \\mathbf{v_1} \\times \\mathbf{v_2} =\n\\begin{vmatrix}\n\\hat{i} & \\hat{j} & \\hat{k} \\\\\n2 & 2 & 5 \\\\\n5 & 5 & 8\n\\end{vmatrix}\n\\]\nCalculating the determinant, we have:\n\\[\n\\mathbf{n} = \\hat{i}(2 \\cdot 8 - 5 \\cdot 5) - \\hat{j}(2 \\cdot 8 - 5 \\cdot 5) + \\hat{k}(2 \\cdot 5 - 2 \\cdot 5)\n\\]\nThis simplifies to:\n\\[\n\\mathbf{n} = \\hat{i}(-9) + \\hat{j}(9) + \\hat{k}(0) = (-9, 9, 0)\n\\]\nThe equation of the plane can be expressed as:\n\\[\nax + by + cz = d\n\\]\nwhere \\(\\mathbf{n} = (a, b, c) = (-9, 9, 0)\\). Using the point \\(P_1(1, 2, 0)\\) to find \\(d\\):\n\\[\n-9(1) + 9(2) + 0(0) = d\n\\]\nThis gives:\n\\[\n-9 + 18 = d \\implies d = 9\n\\]\nThus, the equation of the plane is:\n\\[\n-9x + 9y = 9 \\quad \\text{or} \\quad x - y = -1.\n\\]\n\nFind the directional cosines of the line passing through points \\((1, 1, 1)\\) and \\((4, 5, 6)\\).\n\nSolution:\nThe direction vector of the line is:\n\\[\n\\mathbf{d} = (4 - 1, 5 - 1, 6 - 1) = (3, 4, 5).\n\\]\nThe magnitude of the direction vector is:\n\\[\n|\\mathbf{d}| = \\sqrt{3^2 + 4^2 + 5^2} = \\sqrt{9 + 16 + 25} = \\sqrt{50}.\n\\]\nThe directional cosines are:\n\\[\nl = \\frac{3}{\\sqrt{50}}, \\quad m = \\frac{4}{\\sqrt{50}}, \\quad n = \\frac{5}{\\sqrt{50}}.\n\\]\nThus, the directional cosines are \\(l = \\frac{3}{\\sqrt{50}}\\), \\(m = \\frac{4}{\\sqrt{50}}\\), and \\(n = \\frac{5}{\\sqrt{50}}\\).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Week-1 Summary"
    ]
  },
  {
    "objectID": "2024/index.html",
    "href": "2024/index.html",
    "title": "Computational Mathematics for Data Science",
    "section": "",
    "text": "üì¢ Important\n\n\n\n\n\n(09/10/2024) - üóìÔ∏è Unifying Geometry with Algebra: is now available!\n\nüóìÔ∏è Class Work-1 is now available!\nüóìÔ∏è Class Work-2 is now available!\n\n\n\n\n\nüßëüèª‚Äçüè´ Our Team\n\nCourse ConvenorSupporting Team\n\n\n\nDr.¬†Soman K P  Professor and Dean  Amrita School of Artificial Intelligence üìß kp_soman@amrita.edu ( at )\nOffice Hours:\n\nWhen: 09:00AM-16:30PM\nWhere: ASAI\nHow to book:\n\n\n\n\nDr.¬†Vipin V  Assistant Professor, CEN  üìß EMAIL ( at )\n\nMr.¬†Siju K S  Research Scholar at CEN  üìß EMAIL ( at )\n\n\n\n\n\nüìç Lecture\nFridays 2pm-4pm at PLACE\n\n\nüíª Class Groups\n\nGroup 01\n\nüìÜ Mondays\n‚åö 09:00 - 10:30\nüìç\nüßë‚Äçüè´ TA_NAME\n\n\n\nGroup 02\n\nüìÜ Mondays\n‚åö 10:30 - 12:00\nüìç\nüßë‚Äçüè´ TA_NAME\n\n\n\nGroup 03\n\nüìÜ Fridays\n‚åö 16:00 - 17:30\nüìç\nüßë‚Äçüè´ TA_NAME",
    "crumbs": [
      "üè† Home"
    ]
  },
  {
    "objectID": "2024/weeks/week02/determinants.html",
    "href": "2024/weeks/week02/determinants.html",
    "title": "Computational Mathematics for Data Science",
    "section": "",
    "text": "title: ‚ÄúDeterminants: A Key Value for Square Matrices‚Äù\n\nSection 5 of the ‚ÄúZoomNotes for Linear Algebra‚Äù explores the concept of the determinant, a single number associated with a square matrix that encapsulates crucial information about its properties. The determinant is not only a tool for computation but also a bridge connecting various aspects of linear algebra, including matrix invertibility, solving linear systems, and understanding geometric properties.\n\nDefining Properties and Calculation Methods: From Permutations to Cofactors\nThe determinant of a square matrix A, denoted as det A, can be defined through three fundamental properties:\n\nRow exchange: Exchanging two rows of A changes the sign of det A.\nLinearity in each row: det A is a linear function of each row of A, holding all other rows fixed.\nIdentity matrix: det I = 1, where I is the identity matrix.\n\nThese properties lead to various ways of calculating determinants.\n\nDeterminant of a 2x2 Matrix\nFor a \\(2 \\times 2\\) matrix:\n\\[\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n\\]\nthe determinant is given by:\n\\[\n\\text{det } A = ad - bc\n\\]\nExample: For matrix\n\\[\nA = \\begin{pmatrix} 3 & 5 \\\\ 2 & 4 \\end{pmatrix}\n\\]\nthe determinant is:\n\\[\n\\text{det } A = (3)(4) - (5)(2) = 12 - 10 = 2\n\\]\n\n\nDeterminant of a 3x3 Matrix\nFor a \\(3 \\times 3\\) matrix, the determinant can be calculated using the ‚ÄúBig Formula,‚Äù which sums over all permutations of the columns:\n\\[\nA = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}\n\\]\nThe determinant is given by:\n\\[\n\\text{det } A = a_{11}a_{22}a_{33} - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} - a_{13}a_{22}a_{31} + a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32}\n\\]\nThis formula highlights that each term in the determinant takes one element from each row and each column, with a sign determined by the permutation of the columns.\n\n\nCofactor Expansion\nAnother approach is the cofactor expansion, which expresses the determinant in terms of smaller determinants called cofactors. A cofactor \\(C_{ij}\\) is obtained by removing row \\(i\\) and column \\(j\\) from A and multiplying the determinant of the remaining \\((n-1) \\times (n-1)\\) matrix by \\((-1)^{i+j}\\).\nFor example, the cofactor expansion along row 1 of a \\(3 \\times 3\\) matrix A is:\n\\[\n\\text{det } A = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}\n\\]\n\n\n\nKey Properties and Applications: Invertibility, Cramer‚Äôs Rule, and Volumes\nDeterminants possess several important properties that make them valuable tools in linear algebra:\n\n\n\n\n\n\nKeynotes\n\n\n\nInvertibility: A square matrix A is invertible if and only if det A ‚â† 0. This property stems from the fact that the determinant is related to the volume of the parallelepiped formed by the column vectors of A. If the determinant is zero, the volume collapses to zero, indicating that the columns are linearly dependent, and hence A is not invertible.\nDeterminant of product: $ (AB) = ( A)( B) $ for any two square matrices A and B of the same size.\nDeterminant of transpose: $ A = A^T $.\nDeterminant of inverse: $ (A^{-1}) = $ if A is invertible.\n\n\nThese properties lead to important applications:\n\nCramer‚Äôs Rule\nCramer‚Äôs Rule provides a formula for solving a system of linear equations \\(Ax = \\mathbf{b}\\) when A is invertible. The solution for each component \\(x_i\\) is given by the ratio of two determinants:\n\\[\nx_i = \\frac{\\text{det } B_i}{\\text{det } A}\n\\]\nwhere Bi is the matrix obtained by replacing the \\(i\\)-th column of A with the vector b. However, Cramer‚Äôs Rule is not computationally efficient for large systems due to the determinant calculations involved.\n\n\nVolume Calculation\nThe absolute value of the determinant of a matrix E whose rows (or columns) represent the edges of a parallelepiped in \\(n\\)-dimensional space gives the volume of that parallelepiped. This geometric interpretation connects determinants to the concept of linear transformations and their effect on volumes.\n\n\n\nSpecial Cases: Orthogonal, Triangular, and LU Factorization\nThe following special cases of matrices simplify determinant calculations:\n\nOrthogonal Matrices: For an orthogonal matrix Q, \\(\\text{det } Q = \\pm 1\\) because \\(Q^T Q = I\\) implies \\((\\text{det } Q)^2 = 1\\).\nTriangular Matrices: The determinant of a triangular matrix (upper or lower) is simply the product of its diagonal entries. For example, for an upper triangular matrix:\n\n\\[\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & 6 \\end{pmatrix}\n\\]\nthe determinant is:\n\\[\n\\text{det } A = 1 \\cdot 4 \\cdot 6 = 24\n\\]\n\nLU Factorization: If a matrix A can be factored as \\(A = LU\\), where L is lower triangular and U is upper triangular, then:\n\n\\[\n\\text{det } A = (\\text{det } L)(\\text{det } U) = \\text{product of the pivots in } U.\n\\]\nThis property arises from the fact that elimination operations used to obtain the LU factorization do not change the determinant (except for possible sign changes due to row exchanges).\n\n\nSummary\nOverall, Section 5 of the ‚Äù Computational Linear Algebra‚Äù establishes determinants as fundamental quantities associated with square matrices. Their connection to invertibility, linear systems, and geometric concepts like volumes makes them essential tools for understanding and solving various linear algebra problems. Furthermore, the properties of determinants, especially in special cases like orthogonal and triangular matrices, offer valuable insights and computational advantages in numerous applications."
  },
  {
    "objectID": "2024/weeks/week01/syllabus.html",
    "href": "2024/weeks/week01/syllabus.html",
    "title": "Computational Mathematics for Data Science",
    "section": "",
    "text": "Matrices, Rank, and Gaussian Elimination-vector spaces and subspaces, linear independence, basis and dimension, four fundamental subspaces. Orthogonality - perpendicular vectors and orthogonal subspaces, inner products and projections onto lines, projections and least square applications, orthogonal basis- Gram Schmidt orthogonalization, Eigenvalues and Eigenvectors ‚Äì Singular Value Decomposition, diagonal form of a matrix, Positive Definite Matrices - minima, maxima and saddle points, tests for positive definiteness, semi-definite and indefinite matrices. Introduction -Single Variable calculus and multi-variable calculus Introduction - mathematical optimization, least-squares and linear programming, convex and nonlinear optimization. convex sets, convex optimization problems - optimization problem in standard form, convex optimization problems, quasi-convex optimization, linear optimization, quadratic optimization, generalized inequality constraints, unconstrained minimization- gradient descent method, Conjugate gradient method, steepest descent method, Newton‚Äôs method.\n\nTextbooks / References\n\n\nGilbert Strang, Linear Algebra and its Applications, Fourth Edition, Cambridge University Press. 2009.\nGene H. Golub and V. Van Loan, Matrix Computations, Third Edition, John Hopkins University Press, Baltimore, 1996.\nDavid C. Lay, Linear Algebra and Its Applications, Pearson Addison Wesley, 2002.\nStrang, Gilbert. Linear algebra and learning from data. Cambridge: Wellesley-Cambridge Press, 2019.\nKalyanmoy, Deb. Optimization for engineering design: Algorithms and examples. Prentice-Hall of India Pvt. Limited, 2012.\nChong, Edwin KP, and Stanislaw H. Zak. An introduction to optimization. John Wiley & Sons, 2004.\nBhatti, M. Asghar. Practical Optimization Methods: With Mathematica¬Æ Applications. Springer Science & Business Media, 2012.\nStephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\nLecture notes on optimization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Mathematics for Data Science",
    "section": "",
    "text": "üìë Course Brief\nFocus: the focus of this course is to learn Data Science more effectively\nHow: Problems Solving/ Class work/ Quizes\n\n\nüéØ Learning Objectives\n\nYou will discover the beauty of Mathematical Thinking\nDistinguish Engineering sense from Commonsense\nPractice how to use Mathematical knowledge in Engineering\nDiscover how to use Computational Tools for doing Mathematics\nProceed to learn beyond\n\n\nSelect Academic Year/Term:\n 2024/25"
  },
  {
    "objectID": "2024/weeks/week01/Vectors_matrices.html",
    "href": "2024/weeks/week01/Vectors_matrices.html",
    "title": "Vectors, Matrices, and Operations: A Discussion for Professionals",
    "section": "",
    "text": "A vector is a fundamental object in linear algebra, often visualized as an arrow in space or as an ordered list of numbers.\nA 3-dimensional vector v is commonly represented as:\n\\[\nv = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\n\\]\nThis vector represents a point in three-dimensional space, denoted as \\(\\mathbb{R}^3\\), and can be visualized as an arrow originating from the origin \\((0,0,0)\\) and pointing toward \\((v_1, v_2, v_3)\\).\n\n\nVectors can be manipulated through two basic operations: addition and scalar multiplication.\n\nAddition: Vectors are added component-wise:\n\n\\[\n  \\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ u_3 + v_3 \\end{bmatrix}\n\\]\n\nScalar Multiplication: A vector is multiplied by a scalar \\(\\alpha\\), by multiplying each of its components:\n\n\\[\n  \\alpha \\mathbf{v} = \\alpha \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\alpha v_3 \\end{bmatrix}\n\\]\n\n\n\nA linear combination of vectors is formed by multiplying each vector by a scalar and adding the results. For example, for vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the linear combination is:\n\\[\n2 \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix} - 3 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 6 - 3 \\\\ 8 - 6 \\\\ 10 - 9 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}\n\\]\nLinear combinations are essential because they can represent any point on a line, plane, or higher-dimensional space spanned by the vectors.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Basics of Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week01/Vectors_matrices.html#vectors-the-fundamental-objects",
    "href": "2024/weeks/week01/Vectors_matrices.html#vectors-the-fundamental-objects",
    "title": "Vectors, Matrices, and Operations: A Discussion for Professionals",
    "section": "",
    "text": "A vector is a fundamental object in linear algebra, often visualized as an arrow in space or as an ordered list of numbers.\nA 3-dimensional vector v is commonly represented as:\n\\[\nv = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\n\\]\nThis vector represents a point in three-dimensional space, denoted as \\(\\mathbb{R}^3\\), and can be visualized as an arrow originating from the origin \\((0,0,0)\\) and pointing toward \\((v_1, v_2, v_3)\\).\n\n\nVectors can be manipulated through two basic operations: addition and scalar multiplication.\n\nAddition: Vectors are added component-wise:\n\n\\[\n  \\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ u_3 + v_3 \\end{bmatrix}\n\\]\n\nScalar Multiplication: A vector is multiplied by a scalar \\(\\alpha\\), by multiplying each of its components:\n\n\\[\n  \\alpha \\mathbf{v} = \\alpha \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} = \\begin{bmatrix} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\alpha v_3 \\end{bmatrix}\n\\]\n\n\n\nA linear combination of vectors is formed by multiplying each vector by a scalar and adding the results. For example, for vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the linear combination is:\n\\[\n2 \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix} - 3 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 6 - 3 \\\\ 8 - 6 \\\\ 10 - 9 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}\n\\]\nLinear combinations are essential because they can represent any point on a line, plane, or higher-dimensional space spanned by the vectors.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Basics of Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week01/Vectors_matrices.html#dot-product-measuring-relationships-between-vectors",
    "href": "2024/weeks/week01/Vectors_matrices.html#dot-product-measuring-relationships-between-vectors",
    "title": "Vectors, Matrices, and Operations: A Discussion for Professionals",
    "section": "2. Dot Product: Measuring Relationships Between Vectors",
    "text": "2. Dot Product: Measuring Relationships Between Vectors\nThe dot product of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), denoted as \\(\\mathbf{u} \\cdot \\mathbf{v}\\), is calculated by multiplying corresponding components and summing the results:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_1 v_1 + u_2 v_2 + u_3 v_3\n\\]\nThis product also has a geometric interpretation:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = ||\\mathbf{u}|| ||\\mathbf{v}|| \\cos \\theta\n\\]\nWhere \\(||\\mathbf{u}||\\) and \\(||\\mathbf{v}||\\) are the lengths (or magnitudes) of the vectors, and \\(\\theta\\) is the angle between them.\n\nApplications of Dot Product\n\nLength of a Vector: The length of a vector \\(\\mathbf{v}\\) is \\(||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}\\).\nOrthogonality: Two vectors are orthogonal if their dot product is zero: \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\).\nAngles Between Vectors: The angle between two vectors can be found from the dot product:\n\n\\[\n  \\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{||\\mathbf{u}|| \\, ||\\mathbf{v}||}\n\\]\n\n\nSchwarz Inequality\nThe Schwarz inequality is a fundamental result that arises from the dot product:\n\\[\n|\\mathbf{u} \\cdot \\mathbf{v}| \\leq ||\\mathbf{u}|| \\, ||\\mathbf{v}||\n\\]\nThis inequality bounds the absolute value of the dot product by the product of the vector lengths.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Basics of Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week01/Vectors_matrices.html#matrices-representing-linear-transformations",
    "href": "2024/weeks/week01/Vectors_matrices.html#matrices-representing-linear-transformations",
    "title": "Vectors, Matrices, and Operations: A Discussion for Professionals",
    "section": "3. Matrices: Representing Linear Transformations",
    "text": "3. Matrices: Representing Linear Transformations\nA matrix is a rectangular array of numbers. A matrix with \\(m\\) rows and \\(n\\) columns is called an \\(m \\times n\\) matrix.\n\nMatrix-Vector Multiplication\nMatrices operate on vectors through multiplication. If \\(A\\) is an \\(m \\times n\\) matrix, and \\(\\mathbf{x}\\) is a vector in \\(\\mathbb{R}^n\\), then the product \\(A \\mathbf{x}\\) is an \\(m \\times 1\\) vector.\n\\[\nA = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\]\nThe result is the linear combination of the columns of \\(A\\), scaled by the components of \\(\\mathbf{x}\\).\n\n\nColumn Space\nThe column space of a matrix \\(A\\), denoted as \\(C(A)\\), is the set of all possible vectors that can be formed by matrix-vector multiplication. It captures all possible outputs of the linear transformation represented by \\(A\\).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Basics of Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week01/Vectors_matrices.html#problems-to-solve",
    "href": "2024/weeks/week01/Vectors_matrices.html#problems-to-solve",
    "title": "Vectors, Matrices, and Operations: A Discussion for Professionals",
    "section": "Problems to Solve",
    "text": "Problems to Solve\n\nProblem 1: Vector Addition and Scalar Multiplication\nGiven two vectors \\(\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix}\\), calculate \\(2\\mathbf{u} + 3\\mathbf{v}\\).\n\nHint: Perform scalar multiplication and vector addition.\n\n\n\nProblem 2: Dot Product\nFind the dot product of the vectors \\(\\mathbf{u} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 4 \\\\ 2 \\end{bmatrix}\\).\n\nHint: Use \\(\\mathbf{u} \\cdot \\mathbf{v} = u_1 v_1 + u_2 v_2 + u_3 v_3\\).\n\n\n\nProblem 3: Vector Length and Orthogonality\nDetermine whether the vectors \\(\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -4 \\\\ 0 \\end{bmatrix}\\) are orthogonal.\n\nHint: Compute the dot product and check if it‚Äôs zero.\n\n\n\nProblem 4: Matrix-Vector Multiplication\nGiven \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}\\), calculate \\(A \\mathbf{x}\\).\n\nHint: Multiply each row of the matrix by the vector.\n\n\n\nProblem 5: Column Space\nDetermine whether \\(\\mathbf{b} = \\begin{bmatrix} 5 \\\\ 11 \\end{bmatrix}\\) is in the column space of \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\).\n\nHint: Solve \\(A \\mathbf{x} = \\mathbf{b}\\) for \\(\\mathbf{x}\\).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Basics of Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week01/Vectors_matrices.html#takeaway",
    "href": "2024/weeks/week01/Vectors_matrices.html#takeaway",
    "title": "Vectors, Matrices, and Operations: A Discussion for Professionals",
    "section": "Takeaway",
    "text": "Takeaway\nIn this section, we explored the foundational concepts of vectors, matrices, and their operations. These concepts serve as the building blocks of linear algebra, enabling deeper exploration of topics like linear transformations, eigenvectors, and matrix factorizations. Mastery of these topics is essential for any professional working in data science, machine learning, or related fields.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Basics of Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week01/latexslides.html#course-moderator",
    "href": "2024/weeks/week01/latexslides.html#course-moderator",
    "title": "üóìÔ∏è Week 03 Add-on Course",
    "section": "Course Moderator",
    "text": "Course Moderator\n\n\n\n\n\nProf.¬†(Dr.) Soman K.P. Professor & Dean Amrita School of Artificial Intelligence\n\n\n\n\n\nPh.D., IIT-Kharagpur\nM.Tech. in Reliability Engineering, IIT-Kharagpur\nPost Master Diploma in Statistical Quality Control and Operations Research, Indian Statistical Institute, Calcutta\nBSc Engg. in Electrical Engineering, REC (Now NIT) Kozhikode\n\nDean ASAI  Professor, Electronics and Communication Engineering  World‚Äôs Top 2% Most Influential Scientist"
  },
  {
    "objectID": "2024/weeks/week01/latexslides.html#course-instructors",
    "href": "2024/weeks/week01/latexslides.html#course-instructors",
    "title": "üóìÔ∏è Week 03 Add-on Course",
    "section": "Course Instructors",
    "text": "Course Instructors\n\n\n\n\n\n(Dr.) Vipin V.Assistant Professor, ASAI  PhD in Image Processing  M.Tech. in Artificial Intelligence üìß \n\n\n\n\n\n\nSiju K SAssociate Professor, SAINTGITS  Research Scholar at ASAI  Program Manager, Intel-Unnati M.Sc. in Mathematics üìß \n\n\n\n\n\n\n\n24MA602 ‚ÄìComputational Mathematics for Data Science"
  },
  {
    "objectID": "2024/weeks/week01/page.html",
    "href": "2024/weeks/week01/page.html",
    "title": "üóìÔ∏è Module-1 - Linear System of Equations & Its Solutions",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments, and how we will interact throughout this course.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#lecture-slides",
    "href": "2024/weeks/week01/page.html#lecture-slides",
    "title": "üóìÔ∏è Module-1 - Linear System of Equations & Its Solutions",
    "section": "üë®‚Äçüè´ Lecture Slides",
    "text": "üë®‚Äçüè´ Lecture Slides\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides.\n\n\n\n\nüé• Looking for lecture recordings? You can only find those on Moodle.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#coursework",
    "href": "2024/weeks/week01/page.html#coursework",
    "title": "üóìÔ∏è Module-1 - Linear System of Equations & Its Solutions",
    "section": "‚úçÔ∏è Coursework",
    "text": "‚úçÔ∏è Coursework\nüöß Come back after the lecture to read the coursework instructions for the week üöß",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#recommended-reading",
    "href": "2024/weeks/week01/page.html#recommended-reading",
    "title": "üóìÔ∏è Module-1 - Linear System of Equations & Its Solutions",
    "section": "üìö Recommended Reading",
    "text": "üìö Recommended Reading\n\nCheck the end of slides for the list of references cited in the lecture.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01"
    ]
  },
  {
    "objectID": "2024/weeks/week01/page.html#communication",
    "href": "2024/weeks/week01/page.html#communication",
    "title": "üóìÔ∏è Module-1 - Linear System of Equations & Its Solutions",
    "section": "üìü Communication",
    "text": "üìü Communication\n\nIf you feel like it, introduce yourself in the #introductions channel on Slack",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01"
    ]
  },
  {
    "objectID": "2024/weeks/week01/solution_of_system.html",
    "href": "2024/weeks/week01/solution_of_system.html",
    "title": "Solving Linear Equations: A Core Focus of Linear Algebra",
    "section": "",
    "text": "This section focuses on one of the central applications of linear algebra: solving systems of linear equations. It outlines key concepts and techniques for finding solutions to equations of the form Ax = b, where A is a square n x n matrix, x is a vector of unknowns, and b is a known vector.\n\n\nThe most straightforward way to solve Ax = b is to find the inverse matrix of A, denoted as A^{-1}. If A^{-1} exists, then the solution x is given by:\n\\[\nx = A^{-1}b\n\\]\nHowever, finding the inverse matrix can be computationally expensive, especially for large matrices. Furthermore, not all matrices have inverses. A matrix is invertible if and only if its determinant is non-zero.\n\n\n\nUniqueness: If an inverse matrix exists, it is unique.\nRelationship to Identity Matrix: The product of a matrix and its inverse is the identity matrix (\\(A^{-1}A = I\\) and \\(AA^{-1} = I\\)).\nInvertibility and Linear Independence: A matrix is invertible if and only if its rows (and columns) are linearly independent.\n\n\n\n\n\nSolving systems of linear equations becomes significantly easier when the matrix A is triangular. A triangular matrix has all its entries either above or below the main diagonal equal to zero. There are two types of triangular matrices:\n\nUpper Triangular: All entries below the main diagonal are zero.\nLower Triangular: All entries above the main diagonal are zero.\n\n\n\nConsider an upper triangular system Ux = c:\n\\[\n\\begin{bmatrix}\n2 & 3 & 4 \\\\\n0 & 5 & 6 \\\\\n0 & 0 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} =\n\\begin{bmatrix}\n19 \\\\\n17 \\\\\n14\n\\end{bmatrix}\n\\]\nWe start by solving for \\(x_3\\), then substitute this value into the second equation to find \\(x_2\\), and finally into the first equation to find \\(x_1\\). The solution is:\n\\[\nx_3 = 2, \\quad x_2 = 1, \\quad x_1 = 4\n\\]\n\n\n\n\nElimination is a technique used to transform a general square matrix into an upper triangular matrix, making it easier to solve by back substitution. This process involves a sequence of row operations, aiming to introduce zeros below the diagonal.\n\n\nConsider the matrix:\n\\[\nA = \\begin{bmatrix}\n2 & 3 & 4 \\\\\n4 & 11 & 14 \\\\\n2 & 8 & 17\n\\end{bmatrix}\n\\]\nAfter performing elimination steps, the matrix is transformed into:\n\\[\nU = \\begin{bmatrix}\n2 & 3 & 4 \\\\\n0 & 5 & 6 \\\\\n0 & 0 & 7\n\\end{bmatrix}\n\\]\nThis transformation allows for efficient solving using LU factorization, where:\n\\[\nA = LU\n\\]\n\n\n\n\nDuring elimination, if a diagonal element (pivot) is zero, we perform row exchanges. Row exchanges are represented by permutation matrices. The general solution for Ax = b involves:\n\nFind P such that PA = LU.\nSolve Ly = Pb using forward substitution.\nSolve Ux = y using back substitution.\n\n\n\n\nThe transpose of a matrix A is denoted as A^T. A matrix is symmetric if its transpose equals the original matrix (\\(S^T = S\\)).\n\n\n\nReal Eigenvalues: The eigenvalues of a symmetric matrix are real.\nOrthogonal Eigenvectors: Eigenvectors corresponding to distinct eigenvalues are orthogonal.\n\n\n\n\n\nWhile calculating \\(A^{-1}\\) provides a direct solution, it is often inefficient. For large systems, techniques like LU decomposition with forward and backward substitution are preferred, offering computational efficiency.\nIn summary, Section 2 of ‚ÄúComputational Linear Algebra‚Äù presents key methods for solving linear equations, emphasizing both theoretical foundations and practical efficiency.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Solution of System of Equations"
    ]
  },
  {
    "objectID": "2024/weeks/week01/solution_of_system.html#solving-linear-equations",
    "href": "2024/weeks/week01/solution_of_system.html#solving-linear-equations",
    "title": "Solving Linear Equations: A Core Focus of Linear Algebra",
    "section": "",
    "text": "This section focuses on one of the central applications of linear algebra: solving systems of linear equations. It outlines key concepts and techniques for finding solutions to equations of the form Ax = b, where A is a square n x n matrix, x is a vector of unknowns, and b is a known vector.\n\n\nThe most straightforward way to solve Ax = b is to find the inverse matrix of A, denoted as A^{-1}. If A^{-1} exists, then the solution x is given by:\n\\[\nx = A^{-1}b\n\\]\nHowever, finding the inverse matrix can be computationally expensive, especially for large matrices. Furthermore, not all matrices have inverses. A matrix is invertible if and only if its determinant is non-zero.\n\n\n\nUniqueness: If an inverse matrix exists, it is unique.\nRelationship to Identity Matrix: The product of a matrix and its inverse is the identity matrix (\\(A^{-1}A = I\\) and \\(AA^{-1} = I\\)).\nInvertibility and Linear Independence: A matrix is invertible if and only if its rows (and columns) are linearly independent.\n\n\n\n\n\nSolving systems of linear equations becomes significantly easier when the matrix A is triangular. A triangular matrix has all its entries either above or below the main diagonal equal to zero. There are two types of triangular matrices:\n\nUpper Triangular: All entries below the main diagonal are zero.\nLower Triangular: All entries above the main diagonal are zero.\n\n\n\nConsider an upper triangular system Ux = c:\n\\[\n\\begin{bmatrix}\n2 & 3 & 4 \\\\\n0 & 5 & 6 \\\\\n0 & 0 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} =\n\\begin{bmatrix}\n19 \\\\\n17 \\\\\n14\n\\end{bmatrix}\n\\]\nWe start by solving for \\(x_3\\), then substitute this value into the second equation to find \\(x_2\\), and finally into the first equation to find \\(x_1\\). The solution is:\n\\[\nx_3 = 2, \\quad x_2 = 1, \\quad x_1 = 4\n\\]\n\n\n\n\nElimination is a technique used to transform a general square matrix into an upper triangular matrix, making it easier to solve by back substitution. This process involves a sequence of row operations, aiming to introduce zeros below the diagonal.\n\n\nConsider the matrix:\n\\[\nA = \\begin{bmatrix}\n2 & 3 & 4 \\\\\n4 & 11 & 14 \\\\\n2 & 8 & 17\n\\end{bmatrix}\n\\]\nAfter performing elimination steps, the matrix is transformed into:\n\\[\nU = \\begin{bmatrix}\n2 & 3 & 4 \\\\\n0 & 5 & 6 \\\\\n0 & 0 & 7\n\\end{bmatrix}\n\\]\nThis transformation allows for efficient solving using LU factorization, where:\n\\[\nA = LU\n\\]\n\n\n\n\nDuring elimination, if a diagonal element (pivot) is zero, we perform row exchanges. Row exchanges are represented by permutation matrices. The general solution for Ax = b involves:\n\nFind P such that PA = LU.\nSolve Ly = Pb using forward substitution.\nSolve Ux = y using back substitution.\n\n\n\n\nThe transpose of a matrix A is denoted as A^T. A matrix is symmetric if its transpose equals the original matrix (\\(S^T = S\\)).\n\n\n\nReal Eigenvalues: The eigenvalues of a symmetric matrix are real.\nOrthogonal Eigenvectors: Eigenvectors corresponding to distinct eigenvalues are orthogonal.\n\n\n\n\n\nWhile calculating \\(A^{-1}\\) provides a direct solution, it is often inefficient. For large systems, techniques like LU decomposition with forward and backward substitution are preferred, offering computational efficiency.\nIn summary, Section 2 of ‚ÄúComputational Linear Algebra‚Äù presents key methods for solving linear equations, emphasizing both theoretical foundations and practical efficiency.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 01",
      "üë®‚Äçüè´ Solution of System of Equations"
    ]
  },
  {
    "objectID": "2024/weeks/week02/eigen_value_vector.html",
    "href": "2024/weeks/week02/eigen_value_vector.html",
    "title": "Eigenvalues and Eigenvectors: Unlocking the Essence of Linear Transformations",
    "section": "",
    "text": "Eigenvalues and eigenvectors provide crucial insights into the behavior of linear transformations represented by square matrices. This section explores their definitions, methods for finding them, applications in matrix diagonalization, special cases such as symmetric matrices, and their use in solving differential equations.\n\n\n\n\n\n\nKey Concepts\n\n\n\n\nEigenvalue equation: \\(A\\mathbf{x} = \\lambda \\mathbf{x}\\)\nDiagonalization: Simplifying matrix operations through eigenvectors\nSymmetric matrices: Special properties for real eigenvalues and orthogonal eigenvectors\nApplications: Solving systems of linear differential equations and analyzing matrix structures in engineering\n\n\n\n\n\nEigenvalues and eigenvectors are defined by the fundamental eigenvalue equation:\n\\[\nA \\mathbf{x} = \\lambda \\mathbf{x}\n\\]\nwhere: - \\(A\\) is a square \\(n \\times n\\) matrix, - \\(\\mathbf{x}\\) is a non-zero vector (the eigenvector), - \\(\\lambda\\) is a scalar (the eigenvalue).\nThis equation states that applying the matrix \\(A\\) to an eigenvector results in scaling the vector by the eigenvalue \\(\\lambda\\). Geometrically, the eigenvector does not change its direction, only its magnitude.\nTo calculate eigenvalues, we rearrange the equation as:\n\\[\n(A - \\lambda I)\\mathbf{x} = 0\n\\]\nFor non-trivial solutions, \\((A - \\lambda I)\\) must be singular, meaning:\n\\[\n\\det(A - \\lambda I) = 0\n\\]\nThis is called the characteristic equation. Solving it yields the eigenvalues, and substituting these values back into the equation gives the corresponding eigenvectors.\n\n\nConsider the matrix:\n\\[\nA = \\begin{bmatrix} 0.8 & 0.3 \\\\ 0.2 & 0.7 \\end{bmatrix}\n\\]\nStep 1: Solve the characteristic equation:\n\\[\n\\det(A - \\lambda I) = \\det\\left(\\begin{bmatrix} 0.8 & 0.3 \\\\ 0.2 & 0.7 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\right) = 0\n\\]\n\\[\n\\det\\begin{bmatrix} 0.8 - \\lambda & 0.3 \\\\ 0.2 & 0.7 - \\lambda \\end{bmatrix} = (0.8 - \\lambda)(0.7 - \\lambda) - (0.3)(0.2) = 0\n\\]\n\\[\n(0.8 - \\lambda)(0.7 - \\lambda) - 0.06 = 0\n\\]\nExpanding and solving the quadratic equation:\n\\[\n\\lambda^2 - 1.5\\lambda + 0.5 = 0\n\\]\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 0.5\n\\]\nStep 2: Find the corresponding eigenvectors by solving \\((A - \\lambda I)\\mathbf{x} = 0\\) for each eigenvalue.\nFor \\(\\lambda_1 = 1\\):\n\\[\n(A - I)\\mathbf{x} = \\begin{bmatrix} -0.2 & 0.3 \\\\ 0.2 & -0.3 \\end{bmatrix} \\mathbf{x} = 0\n\\]\nSolving this system gives the eigenvector \\(\\mathbf{x}_1 = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}\\).\nFor \\(\\lambda_2 = 0.5\\):\n\\[\n(A - 0.5I)\\mathbf{x} = \\begin{bmatrix} 0.3 & 0.3 \\\\ 0.2 & 0.2 \\end{bmatrix} \\mathbf{x} = 0\n\\]\nThe eigenvector is \\(\\mathbf{x}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\).\n\nSyntaxDIY\n\n\n# to print a string \"prompt\"\nprint(\"prompt\")\n# to print the value of a variable\nprint(variable)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nEigenvalues and eigenvectors enable diagonalization, a powerful method for simplifying matrix operations, especially powers of matrices. The matrix \\(A\\) can be expressed as:\n\\[\nA = X \\Lambda X^{-1}\n\\]\nwhere \\(X\\) is the matrix of eigenvectors, and \\(\\Lambda\\) is a diagonal matrix of eigenvalues. This transformation allows for easy computation of matrix powers:\n\\[\nA^k = X \\Lambda^k X^{-1}\n\\]\nDiagonalizing \\(A\\) reduces the problem of computing \\(A^k\\) to simple multiplications with diagonal elements.\n\n\nGiven:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}\n\\]\nFind \\(A^4\\) using diagonalization.\nStep 1: Find eigenvalues by solving \\(\\det(A - \\lambda I) = 0\\):\n\\[\n\\det\\begin{bmatrix} 1 - \\lambda & 2 \\\\ 0 & 3 - \\lambda \\end{bmatrix} = (1 - \\lambda)(3 - \\lambda) = 0\n\\]\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 3\n\\]\nStep 2: Find eigenvectors by solving \\((A - \\lambda I)\\mathbf{x} = 0\\).\nFor \\(\\lambda_1 = 1\\): \\(\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\).\nFor \\(\\lambda_2 = 3\\): \\(\\mathbf{x}_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\nStep 3: Form \\(X\\) and \\(\\Lambda\\):\n\\[\nX = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}, \\quad \\Lambda = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix}\n\\]\nStep 4: Compute \\(A^4\\) using \\(A^4 = X \\Lambda^4 X^{-1}\\):\n\\[\n\\Lambda^4 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 81 \\end{bmatrix}\n\\]\nThus, \\(A^4\\) can be computed efficiently.\n\n\n\n\nSymmetric matrices (\\(A = A^T\\)) have special properties:\n\nAll eigenvalues are real.\nEigenvectors corresponding to distinct eigenvalues are orthogonal.\n\nThis leads to the spectral theorem, which states that symmetric matrices can be diagonalized by an orthogonal matrix:\n\\[\nA = Q \\Lambda Q^T\n\\]\nwhere \\(Q\\) is an orthogonal matrix of eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues.\n\n\n\n\n\n\nKey Concepts\n\n\n\n\nSymmetric matrix: \\(A = A^T\\)\nReal eigenvalues: Eigenvalues of symmetric matrices are always real.\nOrthogonal eigenvectors: Eigenvectors corresponding to distinct eigenvalues are orthogonal.\n\n\n\n\n\nConsider the symmetric matrix:\n\\[\nA = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\\]\nFind the eigenvalues and eigenvectors.\nStep 1: Solve the characteristic equation:\n\\[\n\\det(A - \\lambda I) = 0\n\\]\n\\[\n\\det\\begin{bmatrix} 4 - \\lambda & 1 \\\\ 1 & 3 - \\lambda \\end{bmatrix} = (4 - \\lambda)(3 - \\lambda) - 1 = 0\n\\]\nExpanding the quadratic:\n\\[\n\\lambda^2 - 7\\lambda + 11 = 0\n\\]\nThe eigenvalues are \\(\\lambda_1 = 5, \\quad \\lambda_2 = 2\\).\nStep 2: Solve for eigenvectors:\nFor \\(\\lambda_1 = 5\\): \\(\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\nFor \\(\\lambda_2 = 2\\): \\(\\mathbf{x}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\).\nThus, the matrix can be diagonalized as \\(A = Q \\Lambda Q^T\\) where:\n\\[\nQ = \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\Lambda = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix}\n\\]\n\n\n\n\nEigenvalues and eigenvectors are essential in solving systems of linear differential equations of the form:\n\\[\n\\frac{d\\mathbf{u}}{dt} = A\\mathbf{u}\n\\]\nSolutions are of the form \\(\\mathbf{u}(t) = c_1 e^{\\lambda_1 t}\\mathbf{x}_1 + c_2 e^{\\lambda_2 t}\\mathbf{x}_2 + \\cdots\\)\nThis approach simplifies the analysis of the system‚Äôs stability and long-term behavior.\n\n\nConsider the system:\n\\[\n\\frac{d\\mathbf{u}}{dt} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\mathbf{u}\n\\]\nSolve the system using eigenvalue methods.\nStep 1: Find eigenvalues and eigenvectors.\nEigenvalues: \\(\\lambda_1 = 3, \\lambda_2 = -1\\).\nEigenvectors: \\(\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\).\nStep 2: General solution:\n\\[\n\\mathbf{u}(t) = c_1 e^{3t} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + c_2 e^{-t} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\\]\nThis completes the solution of the system.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Unleashes Power of the Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week02/eigen_value_vector.html#introduction",
    "href": "2024/weeks/week02/eigen_value_vector.html#introduction",
    "title": "Eigenvalues and Eigenvectors: Unlocking the Essence of Linear Transformations",
    "section": "",
    "text": "Eigenvalues and eigenvectors provide crucial insights into the behavior of linear transformations represented by square matrices. This section explores their definitions, methods for finding them, applications in matrix diagonalization, special cases such as symmetric matrices, and their use in solving differential equations.\n\n\n\n\n\n\nKey Concepts\n\n\n\n\nEigenvalue equation: \\(A\\mathbf{x} = \\lambda \\mathbf{x}\\)\nDiagonalization: Simplifying matrix operations through eigenvectors\nSymmetric matrices: Special properties for real eigenvalues and orthogonal eigenvectors\nApplications: Solving systems of linear differential equations and analyzing matrix structures in engineering\n\n\n\n\n\nEigenvalues and eigenvectors are defined by the fundamental eigenvalue equation:\n\\[\nA \\mathbf{x} = \\lambda \\mathbf{x}\n\\]\nwhere: - \\(A\\) is a square \\(n \\times n\\) matrix, - \\(\\mathbf{x}\\) is a non-zero vector (the eigenvector), - \\(\\lambda\\) is a scalar (the eigenvalue).\nThis equation states that applying the matrix \\(A\\) to an eigenvector results in scaling the vector by the eigenvalue \\(\\lambda\\). Geometrically, the eigenvector does not change its direction, only its magnitude.\nTo calculate eigenvalues, we rearrange the equation as:\n\\[\n(A - \\lambda I)\\mathbf{x} = 0\n\\]\nFor non-trivial solutions, \\((A - \\lambda I)\\) must be singular, meaning:\n\\[\n\\det(A - \\lambda I) = 0\n\\]\nThis is called the characteristic equation. Solving it yields the eigenvalues, and substituting these values back into the equation gives the corresponding eigenvectors.\n\n\nConsider the matrix:\n\\[\nA = \\begin{bmatrix} 0.8 & 0.3 \\\\ 0.2 & 0.7 \\end{bmatrix}\n\\]\nStep 1: Solve the characteristic equation:\n\\[\n\\det(A - \\lambda I) = \\det\\left(\\begin{bmatrix} 0.8 & 0.3 \\\\ 0.2 & 0.7 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\right) = 0\n\\]\n\\[\n\\det\\begin{bmatrix} 0.8 - \\lambda & 0.3 \\\\ 0.2 & 0.7 - \\lambda \\end{bmatrix} = (0.8 - \\lambda)(0.7 - \\lambda) - (0.3)(0.2) = 0\n\\]\n\\[\n(0.8 - \\lambda)(0.7 - \\lambda) - 0.06 = 0\n\\]\nExpanding and solving the quadratic equation:\n\\[\n\\lambda^2 - 1.5\\lambda + 0.5 = 0\n\\]\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 0.5\n\\]\nStep 2: Find the corresponding eigenvectors by solving \\((A - \\lambda I)\\mathbf{x} = 0\\) for each eigenvalue.\nFor \\(\\lambda_1 = 1\\):\n\\[\n(A - I)\\mathbf{x} = \\begin{bmatrix} -0.2 & 0.3 \\\\ 0.2 & -0.3 \\end{bmatrix} \\mathbf{x} = 0\n\\]\nSolving this system gives the eigenvector \\(\\mathbf{x}_1 = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}\\).\nFor \\(\\lambda_2 = 0.5\\):\n\\[\n(A - 0.5I)\\mathbf{x} = \\begin{bmatrix} 0.3 & 0.3 \\\\ 0.2 & 0.2 \\end{bmatrix} \\mathbf{x} = 0\n\\]\nThe eigenvector is \\(\\mathbf{x}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\).\n\nSyntaxDIY\n\n\n# to print a string \"prompt\"\nprint(\"prompt\")\n# to print the value of a variable\nprint(variable)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nEigenvalues and eigenvectors enable diagonalization, a powerful method for simplifying matrix operations, especially powers of matrices. The matrix \\(A\\) can be expressed as:\n\\[\nA = X \\Lambda X^{-1}\n\\]\nwhere \\(X\\) is the matrix of eigenvectors, and \\(\\Lambda\\) is a diagonal matrix of eigenvalues. This transformation allows for easy computation of matrix powers:\n\\[\nA^k = X \\Lambda^k X^{-1}\n\\]\nDiagonalizing \\(A\\) reduces the problem of computing \\(A^k\\) to simple multiplications with diagonal elements.\n\n\nGiven:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\end{bmatrix}\n\\]\nFind \\(A^4\\) using diagonalization.\nStep 1: Find eigenvalues by solving \\(\\det(A - \\lambda I) = 0\\):\n\\[\n\\det\\begin{bmatrix} 1 - \\lambda & 2 \\\\ 0 & 3 - \\lambda \\end{bmatrix} = (1 - \\lambda)(3 - \\lambda) = 0\n\\]\n\\[\n\\lambda_1 = 1, \\quad \\lambda_2 = 3\n\\]\nStep 2: Find eigenvectors by solving \\((A - \\lambda I)\\mathbf{x} = 0\\).\nFor \\(\\lambda_1 = 1\\): \\(\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\).\nFor \\(\\lambda_2 = 3\\): \\(\\mathbf{x}_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\nStep 3: Form \\(X\\) and \\(\\Lambda\\):\n\\[\nX = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}, \\quad \\Lambda = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix}\n\\]\nStep 4: Compute \\(A^4\\) using \\(A^4 = X \\Lambda^4 X^{-1}\\):\n\\[\n\\Lambda^4 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 81 \\end{bmatrix}\n\\]\nThus, \\(A^4\\) can be computed efficiently.\n\n\n\n\nSymmetric matrices (\\(A = A^T\\)) have special properties:\n\nAll eigenvalues are real.\nEigenvectors corresponding to distinct eigenvalues are orthogonal.\n\nThis leads to the spectral theorem, which states that symmetric matrices can be diagonalized by an orthogonal matrix:\n\\[\nA = Q \\Lambda Q^T\n\\]\nwhere \\(Q\\) is an orthogonal matrix of eigenvectors and \\(\\Lambda\\) is a diagonal matrix of eigenvalues.\n\n\n\n\n\n\nKey Concepts\n\n\n\n\nSymmetric matrix: \\(A = A^T\\)\nReal eigenvalues: Eigenvalues of symmetric matrices are always real.\nOrthogonal eigenvectors: Eigenvectors corresponding to distinct eigenvalues are orthogonal.\n\n\n\n\n\nConsider the symmetric matrix:\n\\[\nA = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\\]\nFind the eigenvalues and eigenvectors.\nStep 1: Solve the characteristic equation:\n\\[\n\\det(A - \\lambda I) = 0\n\\]\n\\[\n\\det\\begin{bmatrix} 4 - \\lambda & 1 \\\\ 1 & 3 - \\lambda \\end{bmatrix} = (4 - \\lambda)(3 - \\lambda) - 1 = 0\n\\]\nExpanding the quadratic:\n\\[\n\\lambda^2 - 7\\lambda + 11 = 0\n\\]\nThe eigenvalues are \\(\\lambda_1 = 5, \\quad \\lambda_2 = 2\\).\nStep 2: Solve for eigenvectors:\nFor \\(\\lambda_1 = 5\\): \\(\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\nFor \\(\\lambda_2 = 2\\): \\(\\mathbf{x}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\).\nThus, the matrix can be diagonalized as \\(A = Q \\Lambda Q^T\\) where:\n\\[\nQ = \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\Lambda = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix}\n\\]\n\n\n\n\nEigenvalues and eigenvectors are essential in solving systems of linear differential equations of the form:\n\\[\n\\frac{d\\mathbf{u}}{dt} = A\\mathbf{u}\n\\]\nSolutions are of the form \\(\\mathbf{u}(t) = c_1 e^{\\lambda_1 t}\\mathbf{x}_1 + c_2 e^{\\lambda_2 t}\\mathbf{x}_2 + \\cdots\\)\nThis approach simplifies the analysis of the system‚Äôs stability and long-term behavior.\n\n\nConsider the system:\n\\[\n\\frac{d\\mathbf{u}}{dt} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\mathbf{u}\n\\]\nSolve the system using eigenvalue methods.\nStep 1: Find eigenvalues and eigenvectors.\nEigenvalues: \\(\\lambda_1 = 3, \\lambda_2 = -1\\).\nEigenvectors: \\(\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\).\nStep 2: General solution:\n\\[\n\\mathbf{u}(t) = c_1 e^{3t} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + c_2 e^{-t} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\\]\nThis completes the solution of the system.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Unleashes Power of the Linear Algebra"
    ]
  },
  {
    "objectID": "2024/weeks/week02/learning_from_data.html",
    "href": "2024/weeks/week02/learning_from_data.html",
    "title": "Unpacking the World of Learning from Data via Gradient Descent",
    "section": "",
    "text": "This section provides a detailed exploration of gradient descent within the realms of linear algebra, matrices, and calculus, foundational areas in both machine learning and deep learning. The focus is on the mathematical formulation of learning functions and the optimization techniques used to train models to extract meaningful patterns from data.\n\n\nA learning function, mathematically represented as:\n\\[ F(\\mathbf{W}, \\mathbf{X}) \\]\nwhere:\n\n\\(\\mathbf{X} \\in \\mathbb{R}^{m \\times n}\\) represents the input data, where each row is a feature vector describing a sample from a dataset.\n\\(\\mathbf{W} \\in \\mathbb{R}^{n \\times k}\\) represents the weight matrix, consisting of learnable parameters that optimize the model‚Äôs performance during training.\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: A learning function maps the input data \\(\\mathbf{X}\\) through a series of transformations to predict an output, using the learnable weight matrix \\(\\mathbf{W}\\).\n\n\nA learning function, such as a neural network, is constructed by stacking multiple layers, each applying two fundamental operations:\n\nLinear Transformation: The transformation at each layer \\(k\\) is described by:\n\\[ \\mathbf{Z}_k = \\mathbf{X}_k \\mathbf{W}_k + \\mathbf{b}_k \\]\nwhere \\(\\mathbf{W}_k \\in \\mathbb{R}^{n_k \\times m_k}\\) is the weight matrix, \\(\\mathbf{b}_k\\) is the bias vector, and \\(\\mathbf{X}_k\\) is the input to the layer.\nNonlinear Activation: After the linear transformation, a nonlinear function such as ReLU (Rectified Linear Unit) is applied:\n\\[ \\mathbf{X}_{k+1} = \\text{ReLU}(\\mathbf{Z}_k) \\]\nwhere ReLU is defined as \\(\\text{ReLU}(z) = \\max(0, z)\\), introducing non-linearity that enables the network to model complex functions.\n\nThe process is repeated across multiple layers, where each output becomes the input for the next layer. The overall function \\(F(\\mathbf{W}, \\mathbf{X})\\) maps the input to the final output through this series of transformations.\n\n\n\nThe landscape of the learning function is a high-dimensional surface, defined by the parameters \\(\\mathbf{W}\\). The activation functions like ReLU create folds or partitions in this surface, dividing it into distinct linear regions. The complexity of the learning function, denoted as \\(r(N, p)\\), grows exponentially with:\n\n\\(N\\): The number of ReLU activations (folds) in the function.\n\\(p\\): The dimensionality of the input data.\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The complexity of the learning function is influenced by the number of ReLU activations and the dimensionality of the input, making the loss landscape more complex.\n\n\nThe total number of distinct linear pieces grows rapidly with the depth (number of layers) and the dimensionality of the input, making the optimization process more challenging.\n\n\n\nThe training of a neural network is framed as an optimization problem, where the goal is to minimize a loss function \\(L(\\mathbf{W})\\) that measures the difference between the predicted values and the actual target values in the training data. Two common loss functions are:\n\nSquare Loss (used in regression tasks):\n\\[ L(\\mathbf{W}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\|\\mathbf{y}_i - \\hat{\\mathbf{y}}_i\\|^2 \\]\nwhere \\(\\mathbf{y}_i\\) is the true value, and \\(\\hat{\\mathbf{y}}_i\\) is the predicted value.\nCross-Entropy Loss (used in classification tasks):\n\\[ L(\\mathbf{W}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\mathbf{y}_i \\log \\hat{\\mathbf{y}}_i + (1 - \\mathbf{y}_i) \\log (1 - \\hat{\\mathbf{y}}_i) \\right] \\]\nwhere \\(\\hat{\\mathbf{y}}_i\\) is the predicted probability for class \\(i\\).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Minimizing the loss function is the core of the optimization process in machine learning. Common loss functions include square loss and cross-entropy loss.\n\n\n\n\n\nThe most common algorithm to minimize the loss function is gradient descent. The gradient \\(\\nabla L(\\mathbf{W})\\) is the derivative of the loss function with respect to the weights \\(\\mathbf{W}\\), providing the direction of steepest ascent. To minimize the loss, weights are updated in the opposite direction:\n\\[ \\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\nabla L(\\mathbf{W}_t) \\]\nwhere:\n\n\\(\\eta\\) is the learning rate, controlling the step size for each update.\n\\(\\nabla L(\\mathbf{W}_t)\\) is the gradient of the loss function at iteration \\(t\\).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Gradient descent is the most widely used algorithm for minimizing the loss function. It updates the weights in the opposite direction of the gradient.\n\n\n\n\n\nGradient descent can converge slowly, especially when traversing narrow valleys in the loss landscape. To address this, momentum accelerates the descent by incorporating information from previous updates:\n\\[ \\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t + (1 - \\beta) \\nabla L(\\mathbf{W}_t) \\]\n\\[ \\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\mathbf{v}_{t+1} \\]\nwhere:\n\n\\(\\mathbf{v}_t\\) is the velocity term, which smooths out the updates.\n\\(\\beta\\) is the momentum parameter (commonly set to 0.9).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Momentum helps accelerate gradient descent by incorporating information from previous steps, leading to faster convergence, especially in complex landscapes.\n\n\nPopular momentum-based algorithms include:\n\nHeavy Ball Method: Incorporates a constant momentum term that helps accelerate convergence.\nADAM (Adaptive Moment Estimation): Adjusts the learning rate by combining gradient information over time using exponential moving averages.\n\n\n\n\nSeveral advanced architectures leverage the power of linear algebra and matrix operations:\n\nConvolutional Neural Networks (CNNs): Utilize convolution operations, represented as matrix multiplications, to capture spatial hierarchies in data such as images:\n\\[ (\\mathbf{X} * \\mathbf{W})_{i,j} = \\sum_{m,n} \\mathbf{X}_{i+m,j+n} \\cdot \\mathbf{W}_{m,n} \\]\nMax-Pooling: Reduces the dimensionality of the feature maps by selecting the maximum value in sub-regions of the input matrix, preserving the most significant features.\nSoftmax Function: Converts the output logits into probabilities, crucial for classification tasks. Given logits \\(z_i\\), the softmax is given by:\n\\[ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} \\]\nResidual Networks (ResNets): Introduce skip connections that bypass certain layers, represented as:\n\\[ \\mathbf{X}_{k+1} = f(\\mathbf{X}_k, \\mathbf{W}_k) + \\mathbf{X}_k \\]\nwhere \\(f(\\mathbf{X}_k, \\mathbf{W}_k)\\) is the transformation applied at layer \\(k\\).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Deep learning architectures like CNNs, Softmax, and ResNets use matrix operations to process data and optimize the learning process.\n\n\n\n\n\nBackpropagation is the algorithm used to efficiently compute gradients for each layer during training. It relies on the chain rule of calculus to propagate gradients backward through the network:\nFor a given layer \\(k\\), the gradient of the loss with respect to the weights is:\n\\[ \\frac{\\partial L}{\\partial \\mathbf{W}_k} = \\frac{\\partial L}{\\partial \\mathbf{Z}_k} \\cdot \\frac{\\partial \\mathbf{Z}_k}{\\partial \\mathbf{W}_k} \\]\nwhere \\(\\mathbf{Z}_k\\) is the output of the linear transformation at layer \\(k\\). The gradients are passed back through the network, updating each layer‚Äôs weights to minimize the overall loss.\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Backpropagation computes gradients efficiently through the chain rule, enabling the optimization of deep networks.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Unpacking the World of Learning from Data"
    ]
  },
  {
    "objectID": "2024/weeks/week02/learning_from_data.html#unpacking-the-world-of-learning-from-data-via-gradient-descent-linear-algebra-and-calculus-perspective",
    "href": "2024/weeks/week02/learning_from_data.html#unpacking-the-world-of-learning-from-data-via-gradient-descent-linear-algebra-and-calculus-perspective",
    "title": "Unpacking the World of Learning from Data via Gradient Descent",
    "section": "",
    "text": "This section provides a detailed exploration of gradient descent within the realms of linear algebra, matrices, and calculus, foundational areas in both machine learning and deep learning. The focus is on the mathematical formulation of learning functions and the optimization techniques used to train models to extract meaningful patterns from data.\n\n\nA learning function, mathematically represented as:\n\\[ F(\\mathbf{W}, \\mathbf{X}) \\]\nwhere:\n\n\\(\\mathbf{X} \\in \\mathbb{R}^{m \\times n}\\) represents the input data, where each row is a feature vector describing a sample from a dataset.\n\\(\\mathbf{W} \\in \\mathbb{R}^{n \\times k}\\) represents the weight matrix, consisting of learnable parameters that optimize the model‚Äôs performance during training.\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: A learning function maps the input data \\(\\mathbf{X}\\) through a series of transformations to predict an output, using the learnable weight matrix \\(\\mathbf{W}\\).\n\n\nA learning function, such as a neural network, is constructed by stacking multiple layers, each applying two fundamental operations:\n\nLinear Transformation: The transformation at each layer \\(k\\) is described by:\n\\[ \\mathbf{Z}_k = \\mathbf{X}_k \\mathbf{W}_k + \\mathbf{b}_k \\]\nwhere \\(\\mathbf{W}_k \\in \\mathbb{R}^{n_k \\times m_k}\\) is the weight matrix, \\(\\mathbf{b}_k\\) is the bias vector, and \\(\\mathbf{X}_k\\) is the input to the layer.\nNonlinear Activation: After the linear transformation, a nonlinear function such as ReLU (Rectified Linear Unit) is applied:\n\\[ \\mathbf{X}_{k+1} = \\text{ReLU}(\\mathbf{Z}_k) \\]\nwhere ReLU is defined as \\(\\text{ReLU}(z) = \\max(0, z)\\), introducing non-linearity that enables the network to model complex functions.\n\nThe process is repeated across multiple layers, where each output becomes the input for the next layer. The overall function \\(F(\\mathbf{W}, \\mathbf{X})\\) maps the input to the final output through this series of transformations.\n\n\n\nThe landscape of the learning function is a high-dimensional surface, defined by the parameters \\(\\mathbf{W}\\). The activation functions like ReLU create folds or partitions in this surface, dividing it into distinct linear regions. The complexity of the learning function, denoted as \\(r(N, p)\\), grows exponentially with:\n\n\\(N\\): The number of ReLU activations (folds) in the function.\n\\(p\\): The dimensionality of the input data.\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: The complexity of the learning function is influenced by the number of ReLU activations and the dimensionality of the input, making the loss landscape more complex.\n\n\nThe total number of distinct linear pieces grows rapidly with the depth (number of layers) and the dimensionality of the input, making the optimization process more challenging.\n\n\n\nThe training of a neural network is framed as an optimization problem, where the goal is to minimize a loss function \\(L(\\mathbf{W})\\) that measures the difference between the predicted values and the actual target values in the training data. Two common loss functions are:\n\nSquare Loss (used in regression tasks):\n\\[ L(\\mathbf{W}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\|\\mathbf{y}_i - \\hat{\\mathbf{y}}_i\\|^2 \\]\nwhere \\(\\mathbf{y}_i\\) is the true value, and \\(\\hat{\\mathbf{y}}_i\\) is the predicted value.\nCross-Entropy Loss (used in classification tasks):\n\\[ L(\\mathbf{W}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\mathbf{y}_i \\log \\hat{\\mathbf{y}}_i + (1 - \\mathbf{y}_i) \\log (1 - \\hat{\\mathbf{y}}_i) \\right] \\]\nwhere \\(\\hat{\\mathbf{y}}_i\\) is the predicted probability for class \\(i\\).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Minimizing the loss function is the core of the optimization process in machine learning. Common loss functions include square loss and cross-entropy loss.\n\n\n\n\n\nThe most common algorithm to minimize the loss function is gradient descent. The gradient \\(\\nabla L(\\mathbf{W})\\) is the derivative of the loss function with respect to the weights \\(\\mathbf{W}\\), providing the direction of steepest ascent. To minimize the loss, weights are updated in the opposite direction:\n\\[ \\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\nabla L(\\mathbf{W}_t) \\]\nwhere:\n\n\\(\\eta\\) is the learning rate, controlling the step size for each update.\n\\(\\nabla L(\\mathbf{W}_t)\\) is the gradient of the loss function at iteration \\(t\\).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Gradient descent is the most widely used algorithm for minimizing the loss function. It updates the weights in the opposite direction of the gradient.\n\n\n\n\n\nGradient descent can converge slowly, especially when traversing narrow valleys in the loss landscape. To address this, momentum accelerates the descent by incorporating information from previous updates:\n\\[ \\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t + (1 - \\beta) \\nabla L(\\mathbf{W}_t) \\]\n\\[ \\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\mathbf{v}_{t+1} \\]\nwhere:\n\n\\(\\mathbf{v}_t\\) is the velocity term, which smooths out the updates.\n\\(\\beta\\) is the momentum parameter (commonly set to 0.9).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Momentum helps accelerate gradient descent by incorporating information from previous steps, leading to faster convergence, especially in complex landscapes.\n\n\nPopular momentum-based algorithms include:\n\nHeavy Ball Method: Incorporates a constant momentum term that helps accelerate convergence.\nADAM (Adaptive Moment Estimation): Adjusts the learning rate by combining gradient information over time using exponential moving averages.\n\n\n\n\nSeveral advanced architectures leverage the power of linear algebra and matrix operations:\n\nConvolutional Neural Networks (CNNs): Utilize convolution operations, represented as matrix multiplications, to capture spatial hierarchies in data such as images:\n\\[ (\\mathbf{X} * \\mathbf{W})_{i,j} = \\sum_{m,n} \\mathbf{X}_{i+m,j+n} \\cdot \\mathbf{W}_{m,n} \\]\nMax-Pooling: Reduces the dimensionality of the feature maps by selecting the maximum value in sub-regions of the input matrix, preserving the most significant features.\nSoftmax Function: Converts the output logits into probabilities, crucial for classification tasks. Given logits \\(z_i\\), the softmax is given by:\n\\[ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} \\]\nResidual Networks (ResNets): Introduce skip connections that bypass certain layers, represented as:\n\\[ \\mathbf{X}_{k+1} = f(\\mathbf{X}_k, \\mathbf{W}_k) + \\mathbf{X}_k \\]\nwhere \\(f(\\mathbf{X}_k, \\mathbf{W}_k)\\) is the transformation applied at layer \\(k\\).\n\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Deep learning architectures like CNNs, Softmax, and ResNets use matrix operations to process data and optimize the learning process.\n\n\n\n\n\nBackpropagation is the algorithm used to efficiently compute gradients for each layer during training. It relies on the chain rule of calculus to propagate gradients backward through the network:\nFor a given layer \\(k\\), the gradient of the loss with respect to the weights is:\n\\[ \\frac{\\partial L}{\\partial \\mathbf{W}_k} = \\frac{\\partial L}{\\partial \\mathbf{Z}_k} \\cdot \\frac{\\partial \\mathbf{Z}_k}{\\partial \\mathbf{W}_k} \\]\nwhere \\(\\mathbf{Z}_k\\) is the output of the linear transformation at layer \\(k\\). The gradients are passed back through the network, updating each layer‚Äôs weights to minimize the overall loss.\n\n\n\n\n\n\nNote\n\n\n\nKey Concept: Backpropagation computes gradients efficiently through the chain rule, enabling the optimization of deep networks.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Unpacking the World of Learning from Data"
    ]
  },
  {
    "objectID": "2024/weeks/week02/orthogonality.html",
    "href": "2024/weeks/week02/orthogonality.html",
    "title": "Orthogonality: A Powerful Tool in Linear Algebra",
    "section": "",
    "text": "This session of the ‚ÄúComputational Linear Algebra‚Äù focuses on orthogonality, a fundamental concept that plays a crucial role in understanding the geometry of vector spaces and solving linear algebra problems. This section explores the orthogonality relationships between the four fundamental subspaces associated with a matrix and introduces the powerful techniques of projections and orthogonal matrices.\n\nOrthogonality of Vectors and Subspaces: Perpendicularity and Independence\nTwo vectors x and y are considered orthogonal if their dot product is zero: xTy = 0. In geometric terms, this means the vectors are perpendicular to each other. The concept extends to complex vectors, where orthogonality is defined using the conjugate transpose: uHv = 0.\n\n\n\n\n\n\nKey Concept\n\n\n\nOrthogonality implies that two vectors are perpendicular, enhancing the understanding of their independence and relationships in vector spaces.\n\n\nThe idea of orthogonality extends to subspaces: Two subspaces V and W are orthogonal if every vector in V is orthogonal to every vector in W.\n\n\n\n\n\n\nKey Concept\n\n\n\nThe orthogonality relationships among the four fundamental subspaces of a matrix A are: - The row space of A and its nullspace are orthogonal. - The column space of A and its left nullspace are orthogonal.\n\n\nThese orthogonality relationships have important consequences:\n\nThey provide a geometric interpretation of the solutions to linear equations. For example, the fact that the row space and nullspace are orthogonal means that any solution to Ax = 0 lies in a direction perpendicular to all the rows of A.\nThey lead to powerful techniques for solving least squares problems, where we seek to find the ‚Äúbest‚Äù solution to an inconsistent system of equations.\n\n\n\nProjections: Finding the Closest Vector in a Subspace\nThe concept of orthogonality leads naturally to the idea of projections onto subspaces. Given a vector b and a subspace S, the projection of b onto S is the vector p in S that is closest to b. The error vector e = b - p is orthogonal to S.\n\n\n\n\n\n\nKey Concept\n\n\n\nThe projection of a vector onto a subspace is the closest point in the subspace to the vector, minimizing the error.\n\n\nThe two important types of projections are:\n\nProjection onto a line: If S is a line spanned by a vector a, the projection of b onto S is given by: \\[\\mathbf{p} = \\frac{\\mathbf{a}^T\\mathbf{b}}{\\mathbf{a}^T\\mathbf{a}} \\mathbf{a}\\]\nProjection onto a subspace spanned by columns of A: If S is the column space of a matrix A, the projection of b onto S is given by: \\[\\mathbf{p} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{b}\\]\nThe matrix P = A(ATA)-1AT is called the projection matrix onto the column space of A. It satisfies the properties P2 = P and PT = P, reflecting that projecting a vector twice onto the same subspace doesn‚Äôt change it.\n\n\n\n\n\n\n\nKey Concept\n\n\n\nThe projection matrix is essential for solving least squares problems, providing the best approximation of a vector within a column space.\n\n\n\n\nLeast Squares Approximations: Handling Inconsistent Systems\nProjections play a crucial role in solving least squares problems, which arise when we have an inconsistent system of equations Ax = b (i.e., a system with no exact solution). In such cases, we seek to find the vector x that minimizes the squared error ||b - Ax||2. This leads to the normal equations:\n\\[\\mathbf{A}^T\\mathbf{A}\\hat{\\mathbf{x}} = \\mathbf{A}^T\\mathbf{b}\\]\n\n\n\n\n\n\nKey Concept\n\n\n\nThe normal equations are derived from minimizing the squared error in least squares problems, leading to the best solution approximation.\n\n\nThe solution \\(\\hat{\\mathbf{x}}\\) gives the coefficients of the projection of b onto the column space of A. This projection, p = A\\(\\hat{\\mathbf{x}}\\), represents the ‚Äúbest‚Äù approximation to b that we can achieve within the column space of A.\n\n\n\n\n\n\nKey Concept\n\n\n\nLeast squares approximations are commonly applied in regression analysis to determine the best-fitting line for data points.\n\n\n\n\nOrthogonal Matrices: Preserving Lengths and Angles\nAn orthogonal matrix is a square matrix Q whose columns are orthonormal. This means that the columns are orthogonal to each other and have unit length. As a result, orthogonal matrices satisfy the property QTQ = I, which implies QT = Q-1.\n\n\n\n\n\n\nKey Concept\n\n\n\nOrthogonal matrices preserve lengths and angles during transformations, making them invaluable in numerical computations.\n\n\nOrthogonal matrices have several important properties:\n\nThey preserve lengths: ||Qx|| = ||x|| for any vector x.\nThey preserve angles: (Qx)T(Qy) = xTy for any vectors x and y.\nThey are computationally efficient to invert, as their inverse is simply their transpose.\n\n\n\nGram-Schmidt Process: Constructing Orthogonal Bases\nThe Gram-Schmidt process provides a systematic way to construct an orthogonal basis for a subspace. Given a set of linearly independent vectors a, b, c, ‚Ä¶, the process generates a set of orthogonal vectors q1, q2, q3, ‚Ä¶ as follows:\n\nq1 = a / ||a||\n\\(q_2=\\frac{\\mathbf{b} - (\\mathbf{q}_1^T\\mathbf{b})\\mathbf{q}_1}{||\\mathbf{b} - (\\mathbf{q}_1^T\\mathbf{b})\\mathbf{q}_1||}\\)\n\\(q_3 =\\frac{\\mathbf{c} - (\\mathbf{q}_1^T\\mathbf{c})\\mathbf{q}_1 - (\\mathbf{q}_2^T\\mathbf{c})\\mathbf{q}_2}{||\\mathbf{c} - (\\mathbf{q}_1^T\\mathbf{c})\\mathbf{q}_1 - (\\mathbf{q}_2^T\\mathbf{c})\\mathbf{q}_2||}\\)\n‚Ä¶\n\nAt each step, we subtract the projection of the current vector onto the subspace spanned by the previous orthogonal vectors, ensuring that the resulting vector is orthogonal to all the previous ones.\n\n\n\n\n\n\nKey Concept\n\n\n\nThe Gram-Schmidt process systematically constructs an orthogonal basis, simplifying many linear algebra computations.\n\n\n\n\nQR Factorization: Expressing a Matrix Using Orthogonal and Triangular Components\nThe Gram-Schmidt process leads to the QR factorization, a powerful matrix factorization that expresses any matrix A as the product of an orthogonal matrix Q and an upper triangular matrix R:\n\\[\\mathbf{A} = \\mathbf{QR}\\]\n\n\n\n\n\n\nKey Concept\n\n\n\nThe QR factorization allows us to express a matrix as a product of orthogonal and triangular components, enhancing numerical stability in computations.\n\n\nThe columns of Q form an orthonormal basis for the column space of A, while R captures the linear combinations of the columns of Q that produce the columns of A.\nThe QR factorization has numerous applications in linear algebra, including:\n\nSolving least squares problems efficiently.\nComputing eigenvalues and eigenvectors.\nPerforming stable numerical computations.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Orthogonality in Vector Space Applications"
    ]
  },
  {
    "objectID": "2024/weeks/week02/svd.html",
    "href": "2024/weeks/week02/svd.html",
    "title": "Singular Value Decomposition (SVD): A Powerful Tool for Matrix Analysis",
    "section": "",
    "text": "Seventh session of the ‚ÄúComputational Linear Algebra‚Äù introduces the Singular Value Decomposition (SVD), a fundamental matrix factorization that decomposes any matrix, even rectangular and non-invertible ones, into a product of three matrices with special properties: two orthogonal matrices and a diagonal matrix. This decomposition has wide-ranging applications in data analysis, image processing, recommendation systems, and other fields.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "2024/weeks/week02/svd.html#applications-dimensionality-reduction-image-compression-and-more",
    "href": "2024/weeks/week02/svd.html#applications-dimensionality-reduction-image-compression-and-more",
    "title": "Singular Value Decomposition (SVD): A Powerful Tool for Matrix Analysis",
    "section": "Applications: Dimensionality Reduction, Image Compression, and More",
    "text": "Applications: Dimensionality Reduction, Image Compression, and More\nThe SVD has numerous applications in various fields, including:\n\nDimensionality Reduction\nBy retaining only the largest singular values and corresponding singular vectors, we can approximate the original matrix \\(A\\) by a lower-rank matrix, effectively reducing the dimensionality of the data. This technique, known as Principal Component Analysis (PCA), is widely used in data analysis and machine learning to extract the most important features from a dataset.\n\\[\nA_k = U_k \\Sigma_k V_k^T\n\\]\nHere, \\(A_k\\) is the approximation of \\(A\\) using only the top \\(k\\) singular values and their corresponding singular vectors.\n\n\nImage Compression\nThe SVD can be used to compress images by representing them as a sum of rank-one matrices, each corresponding to a singular value and its associated singular vectors. By discarding smaller singular values and their corresponding components, we can achieve significant compression while retaining the essential features of the image.\nAn image \\(I\\) can be compressed as:\n\\[\nI \\approx U_k \\Sigma_k V_k^T\n\\]\nwhere \\(k\\) singular values are used for the approximation. This significantly reduces the size of the matrix while preserving the visual quality.\n\n\nRecommendation Systems\nThe SVD is a key technique in matrix factorization methods used in recommendation systems, such as the one employed by Netflix to predict user preferences based on historical data. By decomposing a user-item matrix, the SVD extracts latent features that can be used to predict unknown ratings and recommend items.\nFor example, a user-rating matrix \\(R\\) is factorized as:\n\\[\nR \\approx U \\Sigma V^T\n\\]\nwhere the singular values capture the most significant correlations between users and items, allowing for personalized recommendations.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Week 02",
      "üë®‚Äçüè´ Singular Value Decomposition (SVD)"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "",
    "text": "In this module, we explore LaTeX‚Äôs powerful features for typesetting mathematical expressions. This is one of LaTeX‚Äôs greatest strengths, making it ideal for scientific and engineering documentation. We will cover writing equations and formulas, using LaTeX for complex math, and aligning and referencing equations.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#introduction",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#introduction",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "",
    "text": "In this module, we explore LaTeX‚Äôs powerful features for typesetting mathematical expressions. This is one of LaTeX‚Äôs greatest strengths, making it ideal for scientific and engineering documentation. We will cover writing equations and formulas, using LaTeX for complex math, and aligning and referencing equations.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#writing-equations-and-formulas",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#writing-equations-and-formulas",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Writing Equations and Formulas",
    "text": "Writing Equations and Formulas\nLaTeX offers two primary ways to write mathematical expressions:\n\nInline Math - Used within text, enclosed in $...$.\nDisplayed Math - Standalone equations centered on the page, enclosed with \\[ ... \\] or $$ ... $$.\n\n\nInline Math\nInline math is used for simple equations within a sentence. Place the expression inside $...$ to format it as math.\nExample:\nThe area of a circle is given by $A = \\pi r^2$.\n\n\nDisplayed Math\nDisplayed math is used for larger or more complex expressions that should appear on a separate line.\nExample:\nThe area of a circle can also be represented as:\n\\[\nA = \\pi r^2\n\\]\n\n\nAdditional Examples\n\nCircumference of a circle:\n\n$C = 2\\pi r$.\n\nPythagorean theorem:\n\n\\[\nc^2 = a^2 + b^2\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#the-power-of-latex-for-complex-math",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#the-power-of-latex-for-complex-math",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "The Power of LaTeX for Complex Math",
    "text": "The Power of LaTeX for Complex Math\nLaTeX is especially powerful when handling complex formulas, such as fractions, exponents, roots, and matrices.\n\nFractions, Exponents, and Roots\nFractions are written using \\frac{numerator}{denominator}\nExponents are written as x^2 or e^{x+y}\nRoots can be square roots \\sqrt{x} or nth roots \\sqrt[n]{x}\n\n\n\n\n\n\n\nLaTeX symols\n\n\n\nA comprehensive list of LaTeX symbols is available at (Symbols)https://artofproblemsolving.com/wiki/index.php/LaTeX:Symbols\n\n\nExample:\n\\[\ny = \\frac{ax^2 + bx + c}{d} \\quad \\text{and} \\quad z = \\sqrt[3]{x + y^2}\n\\]",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#matrices",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#matrices",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Matrices",
    "text": "Matrices\nMatrices are fundamental in engineering calculations and can be easily created using the bmatrix, pmatrix, or vmatrix environments.\nExample:\n\\[\n\\begin{bmatrix}\na & b \\\\\nc & d \\\\\n\\end{bmatrix}\n\\]\nExample of a 3x3 matrix:\n\\[\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\]\n\nPractice Complex Math\n\nWrite a fraction using \\frac{a+b}{c+d}.\nCreate a \\(3\\times 3\\) matrix and display it.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#aligning-and-referencing-equations",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#aligning-and-referencing-equations",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Aligning and Referencing Equations",
    "text": "Aligning and Referencing Equations\nWhen working with multiple equations, the align environment helps align equations by the equal sign or another point. You can also label and reference equations throughout the document.\n\nAligning Equations\nThe align environment automatically aligns equations at the & symbol.\nExample:\n\\begin{align}\nf(x) &= ax^2 + bx + c \\\\\ng(x) &= d\\sqrt{x} + e\n\\end{align}\n\n\nReferencing Equations\nTo reference an equation, label it using \\label{label_name} and reference it later using \\eqref{label_name}.\nExample:\n\\begin{equation} \\label{eq:quadratic}\nf(x) = ax^2 + bx + c\n\\end{equation}\n\nThe quadratic equation \\eqref{eq:quadratic} is fundamental in algebra.\n\n\nPractice problem: Align and Reference Equations\nUse the align environment to display and align the following system of equations:\n\\[\\begin{align}\nx + y &= 10 \\\\\nx - y &= 4\n\\end{align}\\]\nLabel one equation and reference it in a sentence below.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-1-writing-basic-equations",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-1-writing-basic-equations",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Task 1: Writing Basic Equations",
    "text": "Task 1: Writing Basic Equations\n\nInline Equation\nWrite an inline equation for the formula of the circumference of a circle.\nDisplayed Equation\nWrite a displayed equation for the Pythagorean theorem.\nQuadratic Formula\nWrite the quadratic formula as a displayed equation.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-2-creating-complex-expressions",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-2-creating-complex-expressions",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Task 2: Creating Complex Expressions",
    "text": "Task 2: Creating Complex Expressions\n\nFraction and Exponent\nCreate a fraction with exponents in both the numerator and the denominator.\nSquare and Cube Roots\nUse square and cube roots in an expression.\nSummation and Product Notations\nWrite the summation notation for the sum of squares from 1 to (n) and the product notation for the product of terms from (k = 1) to (m).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-3-working-with-matrices",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-3-working-with-matrices",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Task 3: Working with Matrices",
    "text": "Task 3: Working with Matrices\n\n2x2 Matrix\nWrite a simple 2x2 matrix.\n3x3 Matrix\nWrite a 3x3 matrix with entries of your choice.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-4-aligning-and-referencing-equations",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#task-4-aligning-and-referencing-equations",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Task 4: Aligning and Referencing Equations",
    "text": "Task 4: Aligning and Referencing Equations\n\nAligned Equations\nUse the align environment to write the following set of equations:\n\n\\(f(x) = ax^2 + bx + c\\)\n\\(g(x) = dx + e\\)\n\nReferencing Equations\nWrite an equation, label it, and then reference it in a subsequent line of text.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#using-biblatex-with-.bib-files",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#using-biblatex-with-.bib-files",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Using biblatex with .bib Files",
    "text": "Using biblatex with .bib Files\nThe biblatex package provides advanced bibliography facilities. Here is an example of how to use it:\n\nCreate a .bib File\nFirst, create a .bib file (e.g., references.bib) with the following content:\n@book{lamport94,\n  author    = {Leslie Lamport},\n  title     = {LaTeX: A Document Preparation System},\n  year      = {1994},\n  publisher = {Addison Wesley},\n  edition   = {2},\n  address   = {Massachusetts}\n}\nInclude the Bibliography in Your LaTeX Document\n\nThen, include the bibliography in your LaTeX document:\n\\usepackage[backend=biber]{biblatex}\n\\addbibresource{references.bib}\n\n\\begin{document}\n\nThis is a citation \\cite{lamport94}.\n\n\\printbibliography\n\n\\end{document}",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#accessing-the-templates",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#accessing-the-templates",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Accessing the Templates",
    "text": "Accessing the Templates\n\nVisit the GitHub Repository\nGo to the provided GitHub link to access a collection of ready-to-use LaTeX templates. These templates are categorized by document type and come with sample content and comments explaining each part of the structure. Link to the GitHub repository: https://github.com/sijuswamy/Common-LaTeX-Templates-for-Engineers\nCloning or Downloading the Repository\nClone the repository or download it as a ZIP file to have local copies of the templates. These templates can then be opened directly in Overleaf or your preferred LaTeX editor.\nExploring Template Files\nEach template includes a README file with an overview of the template‚Äôs structure, styling options, and key sections.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#templates-included",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#templates-included",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Templates Included",
    "text": "Templates Included\nThe repository includes templates for:\n\nTechnical Reports: Comprehensive reports with sections for abstract, introduction, methodology, results, and conclusion.\nAcademic Papers: Templates adhering to common academic journal formats.\nPresentation Slides: Beamer-based templates for creating professional, well-organized slide decks.\nResumes/CVs: Templates optimized for clean, readable layouts ideal for professional profiles.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#customizing-templates",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#customizing-templates",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Customizing Templates",
    "text": "Customizing Templates\nEach template is fully customizable. Participants will learn how to: - Modify title pages, headers, footers, and section headers. - Adjust styling for fonts, colors, and spacing. - Add or remove sections based on specific needs. - Use predefined commands and environments to standardize document styling.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#benefits-of-using-templates",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#benefits-of-using-templates",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Benefits of Using Templates",
    "text": "Benefits of Using Templates\n\nSaves Time: Templates provide a ready-made structure, reducing the time needed for formatting.\nConsistency: Standardized templates help maintain consistent formatting across documents.\nProfessional Appearance: Designed with best practices in mind, the templates ensure a polished, professional look.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_2.html#hands-on-activity",
    "href": "2024/weeks/AddonCourse/LaTeX_module_2.html#hands-on-activity",
    "title": "LaTeX Module 2: Mathematical Typesetting",
    "section": "Hands-On Activity",
    "text": "Hands-On Activity\nParticipants will select a template from the repository, make edits to personalize it, and produce a sample output (e.g., a one-page CV or the first few pages of a technical report). This activity will reinforce template-based LaTeX skills and prepare participants for future document needs.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Mathematical Typesetting"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "In this module, you‚Äôll get hands-on experience with LaTeX basics. We‚Äôll start with a simple document setup and move into creating sections, formatting text, and working with lists. Follow along by copying the code snippets into Overleaf, and watch how LaTeX transforms plain text into a structured, professional-looking document.\n\n\n\n\nOpen Overleaf https://www.overleaf.com/.\nCreate a New Project and choose Blank Project.\nReplace any existing text with the content below.\n\n\n\n\nStart with a basic document structure. Copy the following code into Overleaf as the starting template:\n\\documentclass{article}\n%document preample\n\\title{My First LaTeX Document}\n\\author{Your Name}\n\\date{\\today}\n\\begin{document}\n%document body\n\\maketitle % generating title of the document\n\n\\end{document}\n\n\n\n\n\n\nExplanation:\n\n\n\nThis code sets up the document title, author, and date. \\maketitle generates the title section based on these details.\n\n\n\n\n\nNow, let‚Äôs add an introductory section to the document. Place the following code after \\maketitle:\n\\section*{Introduction}\nWelcome to your first LaTeX document! LaTeX is a typesetting system that makes it easy to produce high-quality documents.\n\n\n\n\n\n\nExplanation:\n\n\n\n\\section*{Introduction} creates an unnumbered section. The * suppresses numbering, making it ideal for introductory or conclusion sections.\n\n\n\n\n\nExpand the document with sections and subsections. Add the following after the Introduction section:\n\\section{Getting Started}\nLaTeX documents are structured in sections and subsections to improve readability.\n\n\\subsection{Installation}\nTo get started with LaTeX, you can either install it locally or use online platforms like Overleaf.\n\n\\subsection{Document Structure}\n\nLaTeX documents are organized into a preamble and a body, where the content goes between \\verb|\\begin{document}| and \\verb|\\end{document}|\n\n\n\n\n\n\nExplanation:\n\n\n\n\\section{...} and \\subsection{...} create numbered sections and subsections. LaTeX automatically numbers these for easy referencing.\n\n\n\n\n\nNow, let‚Äôs add some formatted text within the ‚ÄúGetting Started‚Äù section. Place the following within the Getting Started section, after Document Structure:\n\\section{Text Formatting}\nLaTeX allows you to format text in various styles:\n\n\\textbf{Bold text}\n\n\\textit{Italic text}\n\n\\underline{Underlined text}\n\nThis is a \\verb|\\textbf{\\textit{combination}}| of bold and italic text.\n\n\n\n\n\n\nExplanation:\n\n\n\nThe commands \\textbf{}, \\textit{}, and \\underline{} format text as bold, italic, and underlined. You can also combine these commands to create multiple styles.\n\n\n\n\n\nNext, we‚Äôll create both unordered and ordered lists within the document. Place this after the Text Formatting section:\n\n\\section{Lists}\n\n\\subsection{Unordered List}\n\\begin{itemize}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{itemize}\n\n\\subsection{Ordered List}\n\\begin{enumerate}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{enumerate}\n\n\n\n\n\n\nExplanation:\n\n\n\n\\begin{itemize} creates an unordered (bullet) list, and \\begin{enumerate} creates an ordered (numbered) list. Each list item begins with \\item.\n\nWe can even change the style of individual bullets. The \\item command accepts an optional argument between square brackets that determines the label to be used for that particular item. This is an example of a list with custom bullets:\n\n\n\n\n\n\nLet‚Äôs add a simple table in LaTeX. Insert this after the Lists section:\n\\section{Simple Table}\n\n\\begin{table}[h!]\n    \\centering\n    \\caption{A Simple Table Example}\n    \\label{tab:label1}\n    \\begin{tabular}{|c|c|c|}\n        \\hline\n        Column 1 & Column 2 & Column 3 \\\\\n        \\hline\n        Data 1 & Data 2 & Data 3 \\\\\n        Data 4 & Data 5 & Data 6 \\\\\n        Data 7 & Data 8 & Data 9 \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}\n\n\n\n\n\n\nExplanation:\n\n\n\n\\begin{tabular}{|c|c|c|} creates a 3-column table with vertical lines. Each & separates columns, and each \\\\ begins a new row. The \\hline command creates horizontal lines.\n\nYou can generate LaTeX code for your tables using the online table generators https://www.tablesgenerator.com/\n\n\n\n\n\n\nFor images, you‚Äôll need to use the graphicx package. Add the following code at the top before \\begin{document}, and add the image code below within the document.\n\\usepackage{graphicx} % include it in preample. Required for including images\nInside the document, add after the Table section:\n\\section{Adding Images}\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{example-image} % Replace 'example-image' with your file name\n    \\caption{A Sample Image}\n\\end{figure}\n\n\n\n\n\n\nExplanation:\n\n\n\n\\includegraphics[width=0.5\\textwidth]{file-name} adds an image, with width=0.5\\textwidth resizing it to half the text width. Replace example-image with the actual file name.\n\n\n\n\n\nAdd a footnote and a placeholder for a reference. Place this after the Images section:\n\\section{Footnotes and References}\n\nThis sentence includes a footnote.\\footnote{This is the footnote text.}\n\nTo add references, use \\verb|\\cite{}| with a BibTeX file, which we‚Äôll cover in a future module.\n\n\n\n\n\n\nExplanation:\n\n\n\n\\footnote{} adds a footnote at the bottom of the page. \\cite{} is used for citations, which require a BibTeX file for full functionality.\n\n\n\n\n\nHere‚Äôs how your full document should look after completing Module 1:\n\\documentclass{article}\n\\usepackage{graphicx} % Required for including images\n\n\\begin{document}\n\n\\title{My First LaTeX Document}\n\\author{Your Name}\n\\date{\\today}\n\n\\maketitle\n\n\\section*{Introduction}\nWelcome to your first LaTeX document! LaTeX is a typesetting system that makes it easy to produce high-quality documents.\n\n\\section{Getting Started}\nLaTeX documents are structured in sections and subsections to improve readability.\n\n\\subsection{Installation}\nTo get started with LaTeX, you can either install it locally or use online platforms like Overleaf.\n\n\\subsection{Document Structure}\nLaTeX documents are organized into a preamble and a body, where the content goes between \\verb|\\begin{document}| and \\verb|\\end{document}|.\n\n\\section{Text Formatting}\nLaTeX allows you to format text in various styles:\n\n\\textbf{Bold text}\n\n\\textit{Italic text}\n\n\\underline{Underlined text}\n\nThis is a \\verb|\\textbf{\\textit{combination}}| of bold and italic text.\n\n\\section{Lists}\n\n\\subsection{Unordered List}\n\\begin{itemize}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{itemize}\n\n\\subsection{Ordered List}\n\\begin{enumerate}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{enumerate}\n\n\\section{Simple Table}\n\n\\begin{table}[h!]\n    \\centering\n    \\begin{tabular}{|c|c|c|}\n        \\hline\n        Column 1 & Column 2 & Column 3 \\\\\n        \\hline\n        Data 1 & Data 2 & Data 3 \\\\\n        Data 4 & Data 5 & Data 6 \\\\\n        Data 7 & Data 8 & Data 9 \\\\\n        \\hline\n    \\end{tabular}\n    \\caption{A Simple Table Example}\n\\end{table}\n\n\\section{Adding Images}\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{example-image} % Replace 'example-image' with your file name\n    \\caption{A Sample Image}\n\\end{figure}\n\n\\section{Footnotes and References}\n\nThis sentence includes a footnote.\\footnote{This is the footnote text.}\n\nTo add references, use \\verb|\\cite{}| with a BibTeX file, which we‚Äôll cover in a future module.\n\n\\end{document}",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#getting-started-with-latex",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#getting-started-with-latex",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Open Overleaf https://www.overleaf.com/.\nCreate a New Project and choose Blank Project.\nReplace any existing text with the content below.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#basic-document-setup",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#basic-document-setup",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Start with a basic document structure. Copy the following code into Overleaf as the starting template:\n\\documentclass{article}\n%document preample\n\\title{My First LaTeX Document}\n\\author{Your Name}\n\\date{\\today}\n\\begin{document}\n%document body\n\\maketitle % generating title of the document\n\n\\end{document}\n\n\n\n\n\n\nExplanation:\n\n\n\nThis code sets up the document title, author, and date. \\maketitle generates the title section based on these details.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-an-introduction-section",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-an-introduction-section",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Now, let‚Äôs add an introductory section to the document. Place the following code after \\maketitle:\n\\section*{Introduction}\nWelcome to your first LaTeX document! LaTeX is a typesetting system that makes it easy to produce high-quality documents.\n\n\n\n\n\n\nExplanation:\n\n\n\n\\section*{Introduction} creates an unnumbered section. The * suppresses numbering, making it ideal for introductory or conclusion sections.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-structured-sections-and-subsections",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-structured-sections-and-subsections",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Expand the document with sections and subsections. Add the following after the Introduction section:\n\\section{Getting Started}\nLaTeX documents are structured in sections and subsections to improve readability.\n\n\\subsection{Installation}\nTo get started with LaTeX, you can either install it locally or use online platforms like Overleaf.\n\n\\subsection{Document Structure}\n\nLaTeX documents are organized into a preamble and a body, where the content goes between \\verb|\\begin{document}| and \\verb|\\end{document}|\n\n\n\n\n\n\nExplanation:\n\n\n\n\\section{...} and \\subsection{...} create numbered sections and subsections. LaTeX automatically numbers these for easy referencing.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#text-formatting",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#text-formatting",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Now, let‚Äôs add some formatted text within the ‚ÄúGetting Started‚Äù section. Place the following within the Getting Started section, after Document Structure:\n\\section{Text Formatting}\nLaTeX allows you to format text in various styles:\n\n\\textbf{Bold text}\n\n\\textit{Italic text}\n\n\\underline{Underlined text}\n\nThis is a \\verb|\\textbf{\\textit{combination}}| of bold and italic text.\n\n\n\n\n\n\nExplanation:\n\n\n\nThe commands \\textbf{}, \\textit{}, and \\underline{} format text as bold, italic, and underlined. You can also combine these commands to create multiple styles.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#creating-lists",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#creating-lists",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Next, we‚Äôll create both unordered and ordered lists within the document. Place this after the Text Formatting section:\n\n\\section{Lists}\n\n\\subsection{Unordered List}\n\\begin{itemize}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{itemize}\n\n\\subsection{Ordered List}\n\\begin{enumerate}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{enumerate}\n\n\n\n\n\n\nExplanation:\n\n\n\n\\begin{itemize} creates an unordered (bullet) list, and \\begin{enumerate} creates an ordered (numbered) list. Each list item begins with \\item.\n\nWe can even change the style of individual bullets. The \\item command accepts an optional argument between square brackets that determines the label to be used for that particular item. This is an example of a list with custom bullets:",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-a-simple-table",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-a-simple-table",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Let‚Äôs add a simple table in LaTeX. Insert this after the Lists section:\n\\section{Simple Table}\n\n\\begin{table}[h!]\n    \\centering\n    \\caption{A Simple Table Example}\n    \\label{tab:label1}\n    \\begin{tabular}{|c|c|c|}\n        \\hline\n        Column 1 & Column 2 & Column 3 \\\\\n        \\hline\n        Data 1 & Data 2 & Data 3 \\\\\n        Data 4 & Data 5 & Data 6 \\\\\n        Data 7 & Data 8 & Data 9 \\\\\n        \\hline\n    \\end{tabular}\n\\end{table}\n\n\n\n\n\n\nExplanation:\n\n\n\n\\begin{tabular}{|c|c|c|} creates a 3-column table with vertical lines. Each & separates columns, and each \\\\ begins a new row. The \\hline command creates horizontal lines.\n\nYou can generate LaTeX code for your tables using the online table generators https://www.tablesgenerator.com/",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-images",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-images",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "For images, you‚Äôll need to use the graphicx package. Add the following code at the top before \\begin{document}, and add the image code below within the document.\n\\usepackage{graphicx} % include it in preample. Required for including images\nInside the document, add after the Table section:\n\\section{Adding Images}\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{example-image} % Replace 'example-image' with your file name\n    \\caption{A Sample Image}\n\\end{figure}\n\n\n\n\n\n\nExplanation:\n\n\n\n\\includegraphics[width=0.5\\textwidth]{file-name} adds an image, with width=0.5\\textwidth resizing it to half the text width. Replace example-image with the actual file name.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-footnotes-and-simple-references",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#adding-footnotes-and-simple-references",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Add a footnote and a placeholder for a reference. Place this after the Images section:\n\\section{Footnotes and References}\n\nThis sentence includes a footnote.\\footnote{This is the footnote text.}\n\nTo add references, use \\verb|\\cite{}| with a BibTeX file, which we‚Äôll cover in a future module.\n\n\n\n\n\n\nExplanation:\n\n\n\n\\footnote{} adds a footnote at the bottom of the page. \\cite{} is used for citations, which require a BibTeX file for full functionality.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#full-document-code",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#full-document-code",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "",
    "text": "Here‚Äôs how your full document should look after completing Module 1:\n\\documentclass{article}\n\\usepackage{graphicx} % Required for including images\n\n\\begin{document}\n\n\\title{My First LaTeX Document}\n\\author{Your Name}\n\\date{\\today}\n\n\\maketitle\n\n\\section*{Introduction}\nWelcome to your first LaTeX document! LaTeX is a typesetting system that makes it easy to produce high-quality documents.\n\n\\section{Getting Started}\nLaTeX documents are structured in sections and subsections to improve readability.\n\n\\subsection{Installation}\nTo get started with LaTeX, you can either install it locally or use online platforms like Overleaf.\n\n\\subsection{Document Structure}\nLaTeX documents are organized into a preamble and a body, where the content goes between \\verb|\\begin{document}| and \\verb|\\end{document}|.\n\n\\section{Text Formatting}\nLaTeX allows you to format text in various styles:\n\n\\textbf{Bold text}\n\n\\textit{Italic text}\n\n\\underline{Underlined text}\n\nThis is a \\verb|\\textbf{\\textit{combination}}| of bold and italic text.\n\n\\section{Lists}\n\n\\subsection{Unordered List}\n\\begin{itemize}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{itemize}\n\n\\subsection{Ordered List}\n\\begin{enumerate}\n    \\item First item\n    \\item Second item\n    \\item Third item\n\\end{enumerate}\n\n\\section{Simple Table}\n\n\\begin{table}[h!]\n    \\centering\n    \\begin{tabular}{|c|c|c|}\n        \\hline\n        Column 1 & Column 2 & Column 3 \\\\\n        \\hline\n        Data 1 & Data 2 & Data 3 \\\\\n        Data 4 & Data 5 & Data 6 \\\\\n        Data 7 & Data 8 & Data 9 \\\\\n        \\hline\n    \\end{tabular}\n    \\caption{A Simple Table Example}\n\\end{table}\n\n\\section{Adding Images}\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{example-image} % Replace 'example-image' with your file name\n    \\caption{A Sample Image}\n\\end{figure}\n\n\\section{Footnotes and References}\n\nThis sentence includes a footnote.\\footnote{This is the footnote text.}\n\nTo add references, use \\verb|\\cite{}| with a BibTeX file, which we‚Äôll cover in a future module.\n\n\\end{document}",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-1-modify-document-title-and-author",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-1-modify-document-title-and-author",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 1: Modify Document Title and Author",
    "text": "Task 1: Modify Document Title and Author\n\nChange the title to something unique, like ‚ÄúLearning LaTeX with Confidence.‚Äù\nUpdate the author with your full name and add a brief subtitle to introduce your document‚Äôs purpose.\nSet a custom date instead of using \\today (e.g., your birthdate or today‚Äôs date in a different format).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-2-add-a-new-section-with-subsections",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-2-add-a-new-section-with-subsections",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 2: Add a New Section with Subsections",
    "text": "Task 2: Add a New Section with Subsections\n\nCreate a new section called Learning LaTeX.\nAdd two subsections titled Why Learn LaTeX? and Benefits of Using LaTeX.\nIn each subsection, write two sentences about the importance of LaTeX for engineering professionals.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-3-experiment-with-text-formatting",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-3-experiment-with-text-formatting",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 3: Experiment with Text Formatting",
    "text": "Task 3: Experiment with Text Formatting\n\nIn the Learning LaTeX section, add some text with bold, italic, and underlined formatting.\nTry combining formatting, such as bold and italic text together.\nAdd a quote block using \\begin{quote} ... \\end{quote} with a famous quote about learning or knowledge.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-4-create-lists",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-4-create-lists",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 4: Create Lists",
    "text": "Task 4: Create Lists\n\nIn the Benefits of Using LaTeX subsection, create an unordered list highlighting three advantages of using LaTeX.\nAdd an ordered list in the same section with three steps to start using LaTeX (e.g., choosing a platform, learning syntax basics, etc.).",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-5-design-a-table",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-5-design-a-table",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 5: Design a Table",
    "text": "Task 5: Design a Table\n\nAdd a section called Comparison Table.\nCreate a table with two columns: Feature and Description.\nList three LaTeX features, with a brief description of each.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-6-insert-an-image",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-6-insert-an-image",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 6: Insert an Image",
    "text": "Task 6: Insert an Image\n\nAdd an image to your document in a new section titled Visual Elements.\nSearch for a free image related to ‚Äútechnology‚Äù or ‚Äúengineering‚Äù online and upload it to Overleaf.\nUse \\includegraphics to display it at a width of 0.7 times the text width, and add a caption.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-7-experiment-with-footnotes",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-7-experiment-with-footnotes",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 7: Experiment with Footnotes",
    "text": "Task 7: Experiment with Footnotes\n\nAdd a footnote in the Learning LaTeX section explaining a technical term (e.g., ‚Äútypesetting‚Äù).\nTry placing a second footnote in the Benefits of Using LaTeX subsection, providing additional information.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-8-add-a-customized-title-page",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-8-add-a-customized-title-page",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 8: Add a Customized Title Page",
    "text": "Task 8: Add a Customized Title Page\n\nResearch \\titlepage and try creating a dedicated title page by adding \\maketitle on a new page.\nExperiment with the layout by moving \\maketitle to different positions in the document.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-9-customize-the-table-of-contents-optional",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-9-customize-the-table-of-contents-optional",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 9: Customize the Table of Contents (Optional)",
    "text": "Task 9: Customize the Table of Contents (Optional)\n\nAdd \\tableofcontents before the first section to generate an automatic table of contents.\nExperiment with \\section*{...} vs.¬†\\section{...} in one of the sections to see how it impacts the table of contents.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  },
  {
    "objectID": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-10-reflective-practice-optional",
    "href": "2024/weeks/AddonCourse/LaTeX_module_1.html#task-10-reflective-practice-optional",
    "title": "Module 1: Introduction to LaTeX and Setup",
    "section": "Task 10: Reflective Practice (Optional)",
    "text": "Task 10: Reflective Practice (Optional)\n\nAdd a section at the end titled Reflections.\nWrite a short paragraph about what you‚Äôve learned and any questions you have about LaTeX.\nFormat this paragraph with any styles you learned, like bold, italic, and quotes.",
    "crumbs": [
      "üóìÔ∏è Weeks",
      "Add-On-Course",
      "üë®‚Äçüè´ Introduction to LaTeX and Setup"
    ]
  }
]